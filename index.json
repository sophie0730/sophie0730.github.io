[{"content":"來實作一個container吧 延續上篇的動手用Golang實作一個container - 概念篇，了解container的底層技術是如何實踐之後，我們就可以開始使用Golang來做出屬於我們自己的container了。\n以Docker為例，當我們要啟動一個container的時候，會使用這個指令：\ndocker container run \u0026lt;image-name\u0026gt; [cmd] 以此為發想點，當我想要使用我的程式碼啟動container時，他長得會像這樣子：\ngo run main.go run [cmd] 在這裡，我們會分別定義兩種function：\nrun() : parent process要執行的function。負責創建child process及並配置其運行的環境（如namespace）。 child() : child process要執行的function。負責管理在container環境中，要如何運行用戶端所指定的命令。 而must()function則會作為error handler使用。\nfunc main() { switch os.Args[1] { case \u0026#34;run\u0026#34;: run() case \u0026#34;child\u0026#34;: child(); default: panic(\u0026#34;what??\u0026#34;) } } func must(err error) { if err != nil { panic(err) } } 這邊還蠻想提一下os.Args這個指令。os.Args 是Golang裡面用來儲存命令行參數的一個變數。如果我們只單印出os.Args，他會長的像這樣：\nsophie@Sophie-Desktop:~/go-container$ go run test.go run echo \u0026#39;hi\u0026#39; [/tmp/go-build3547203082/b001/exe/test run echo hi] 第一行是我執行Go程式碼的指令，第二行是os.Args印出來的結果。到這裡會發現為什麼go run test.go 不但沒有被儲存在os.Args中，反而還多了一串類似像檔案路徑的東西？\n這是因為當我們使用go run來運行程式時，他會先編譯我們指定的程式碼檔案，創建一個臨時的執行檔，它實際運行的其實是這個臨時創建出來的執行檔。\n因此，這也能說明為什麼go run test.go直接被這個臨時執行檔的路徑給取代了。基於這個架構，我們使用os.Args[1]的元素就能判斷是否為執行container的命令行。\nParent Process run()這個function定義了parent process的行為。在這裡，我們會需要做到以下這幾件事：\n創建一個child process - 這個child process將會是這個container裡面的PID 1 process。 使用這個child process執行命令行中，run後面的指令。 為這個child process創建一個新的PID namespace、UTS namespace （實際上大家使用的container技術如Docker，可能會有更多的namespace來做更加嚴實的隔離，在這裡我們先使用兩個namespace作為實驗） func run() { cmd := exec.Command(\u0026#34;/proc/self/exe\u0026#34;, append([]string{\u0026#34;child\u0026#34;}, os.Args[2:]...)...) cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr //create a namespace and new pid cmd.SysProcAttr = \u0026amp;syscall.SysProcAttr{ Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID, } must(cmd.Run()) } 這邊可以注意到我使用exec.Command呼叫了一個child process，這個child process會使用我們剛剛編譯過的臨時執行檔，而且會創建一個新的命令行指令，且以\u0026quot;child\u0026quot;作為開頭，而後面的arguments則會與parent process的指令相同。\n講的可能有些饒口，意思是，當parent process執行的命令行指令為go run main.go run /bin/bash，那麼現在這個child process則會執行/tmp/go-build3547203082/b001/exe/test child /bin/bash。\n到這邊，我們就可以開始定義child process接下來的行為了。\nChild Process (container中的PID 1 Process) 我們可以先在這個function埋一個log，來看這個child process的PID是否真的為1。\nfmt.Printf(\u0026#34;running %v as PID %d\\n\u0026#34;, os.Args[2:], os.Getpid()) 在child()function裡面，可以實際來執行剛剛所輸入的命令行指令/bin/bash。\ncmd := exec.Command(os.Args[2], os.Args[3:]... ) cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr must(cmd.Run()) 完成後，到這裡我們就可以先試跑看看程式碼了。 可以觀察到，/bin/bash不僅被執行，且PID真的是1！接著，我們可以來試試看現在是否處在container的隔離環境之中：\n我先在這個container裡面，把我的hostname從Sophie-Desktop替換成test-sophie： 退出container之後，在host觀察hostname，會發現hostname並沒有被更動！UTS namespace的確有發揮作用。 到這裡，一個簡易版的container（看似）就完成了。\n編按：這邊要提醒讀者，這裡的hostname之所以能做到隔離，是因為在前面我們加上了UTS namespace syscall.CLONE_NEWUTS，在這次實驗中，我們只有加了PID和UTS這兩個比較好觀察到的namespace，因此只能在Hostname以及PID的部分觀察到隔離的效果！\n檔案路徑與Process的可見性 為什麼上一個章節會說「看似」完成呢？實際上，我們再回到container中，會發現我們看得到host的檔案以及正在運行的process： 這並不符合我們對於container的理解，對吧？\n我們先來釐清一下為什麼會觀察到這樣的狀況。我們剛剛使用namespace創建了獨立的環境，來讓process運行，但實際上以目前的程式碼來看，container所使用的檔案系統和host會是同一個。為了要避免container裡面的process直接去訪問、操作位在host的檔案，我們可以使用先建造一個具有完整Linux系統的子目錄，再使用上一篇所提到chroot來改變container的根目錄位址。\n在Linux環境中，可以使用debootstrap指令來創建這個子目錄，以下指令是以AMD64的晶片架構、Ubuntu 22.04為例。\nsudo apt-get install debootstrap sudo debootstrap --arch=amd64 jammy /home/rootfs http://archive.ubuntu.com/ubuntu/ 接著就能看到在home/rootfs底下，就有一個基本的Linux檔案架構了。回到程式，便可以指定這個路徑，讓它成為container的根目錄。 must(syscall.Chroot(\u0026#34;/home/rootfs\u0026#34;)) must(os.Chdir(\u0026#34;/\u0026#34;)) 編按：如果使用這樣的方式，container對文件的修改會直接影響到host的檔案。最好的方法，還是要回歸上一篇有稍微提及的聯合檔案系統，將文件分為唯讀層、可寫層，所有的修改都應只在可寫層上進行。\n再回頭來說說Process。在Linux中，/proc負責存取系統中每一個process和thread的狀態。回想前一個章節，當我們還在和host共用檔案目錄時，自然也讀取到了host的/proc檔案，連帶讀取到host的所有process了。\n我們剛剛製作的小型Linux檔案系統並沒有包含/proc檔案系統，因此需要手動另外掛載：\nmust(syscall.Mount(\u0026#34;proc\u0026#34;, \u0026#34;proc\u0026#34;, \u0026#34;proc\u0026#34;, 0, \u0026#34;\u0026#34;)) 完成後，再回頭執行ps aux，就會出現預期中的效果了。 結語 此次透過實作一個container，來更加了解container的底層技術。不過，礙於篇幅及時間有限，實做出來的東西僅做實驗使用，並沒有做到非常完整的隔離與資源限制，或許都能成為下次努力精進的方向。\n最後，不免俗地於文末附上完整的程式碼。（當然，僅供學習使用，其嚴謹度當然是不能夠使用在production環境！）\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;os/exec\u0026#34; \u0026#34;syscall\u0026#34; ) func main() { switch os.Args[1] { case \u0026#34;run\u0026#34;: run() case \u0026#34;child\u0026#34;: child() default: panic(\u0026#34;what??\u0026#34;) } } func run() { cmd := exec.Command(\u0026#34;/proc/self/exe\u0026#34;, append([]string{\u0026#34;child\u0026#34;}, os.Args[2:]...)...) cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr //create a namespace and new pid cmd.SysProcAttr = \u0026amp;syscall.SysProcAttr{ Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID, } must(cmd.Run()) } func child() { fmt.Printf(\u0026#34;running %v as PID %d\\n\u0026#34;, os.Args[2:], os.Getpid()) cmd := exec.Command(os.Args[2], os.Args[3:]... ) cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr //create file system for container must(syscall.Chroot(\u0026#34;/home/rootfs\u0026#34;)) must(os.Chdir(\u0026#34;/\u0026#34;)) //mount /proc must(syscall.Mount(\u0026#34;proc\u0026#34;, \u0026#34;proc\u0026#34;, \u0026#34;proc\u0026#34;, 0, \u0026#34;\u0026#34;)) must(cmd.Run()) } func must(err error) { if err != nil { panic(err) } } Reference 此篇文章受這部影片所啟發：Building a container from scratch in Go - Liz Rice (Microscaling Systems)\n","permalink":"https://sophie0730.github.io/posts/2024/06/build_container_by_go_practice/","summary":"來實作一個container吧 延續上篇的動手用Golang實作一個container - 概念篇，了解container的底層技術是如何實踐之後，我們就可以開始使用Golang來做出屬於我們自己的container了。\n以Docker為例，當我們要啟動一個container的時候，會使用這個指令：\ndocker container run \u0026lt;image-name\u0026gt; [cmd] 以此為發想點，當我想要使用我的程式碼啟動container時，他長得會像這樣子：\ngo run main.go run [cmd] 在這裡，我們會分別定義兩種function：\nrun() : parent process要執行的function。負責創建child process及並配置其運行的環境（如namespace）。 child() : child process要執行的function。負責管理在container環境中，要如何運行用戶端所指定的命令。 而must()function則會作為error handler使用。\nfunc main() { switch os.Args[1] { case \u0026#34;run\u0026#34;: run() case \u0026#34;child\u0026#34;: child(); default: panic(\u0026#34;what??\u0026#34;) } } func must(err error) { if err != nil { panic(err) } } 這邊還蠻想提一下os.Args這個指令。os.Args 是Golang裡面用來儲存命令行參數的一個變數。如果我們只單印出os.Args，他會長的像這樣：\nsophie@Sophie-Desktop:~/go-container$ go run test.go run echo \u0026#39;hi\u0026#39; [/tmp/go-build3547203082/b001/exe/test run echo hi] 第一行是我執行Go程式碼的指令，第二行是os.Args印出來的結果。到這裡會發現為什麼go run test.","title":"動手用Golang實作一個container - 實作篇"},{"content":"前言 開始第一份工作以後，真切體會到容器化技術的強大與方便之處，工作中處處離不開container，但自己又真的懂它幫我們做了什麼事情嗎？為了更了解容器化技術的底層原理，那不如就自己來做一個container看看好了！\nCNCF的開發專案大多由Golang寫成，同時做為一個語法簡潔、易讀、擁有強大併發處理能力的語言，我相信使用它來建構容器等系統工具是個好選擇，因此本文將會以Golang作為程式碼的範例。\n什麼時候會需要用到container? 要回答這個問題，我們先來看看Docker官方對於container的解釋：\nA container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another.\n一年前我還是個連docker都沒聽過的程式小白，當我想把網站部署到雲端上時，我的作法就是直接把github的程式碼clone到我的機器上，直接加裝任何程式碼所需要的套件及程式語言。現在回想起來，直接在機器上面運行程式碼實在有太多風險了：錯誤的程式碼可能造成系統損壞、資源過度消耗，甚至是運行的程式碼可能包含惡意程式而導致安全漏洞……除此之外，直接在機器上加裝一堆套件也讓環境變得又髒又亂，更別說在多人協作的開發場合中，建置環境時也很常發生「為什麼程式碼在你的電腦可以跑，我的不行？」的惱人狀況。\n基於以上痛點，我們再回頭來看container的定義：container是一個標準的軟體單位，它把程式碼以及所需要的環境與依賴項目給一起打包，讓整個應用程式可以快速地被搬運到另一個運算環境並且可靠地運行。有了container，我們不但可以避免直接運行程式碼的風險，開發環境與生產環境也都會變得乾淨許多。\n那麼，container是怎麼做到的？以上面的定義來看，container做到了環境打包、隔離，這兩個功能對應到的linux技術即是filesystem以及namespace，除此之外，host也需要去管理與限制container可以使用的資源，而這部分就屬於cgroups的範疇。因此，為了更了解container的底層原理，以下將會敘述這三個技術是怎麼成就container的。\nNamespace 當我們運行一個container時，會發現在container當中，我們只能看到在container裡運行的process，如果先前對container有一些認識，大概會知道container是使用命名空間來做到隔離。\n我們直接來看Linux manual page中對於Namespace的解釋：\nA namespace wraps a global system resource in an abstraction thatmakes it appear to the processes within the namespace that theyhave their own isolated instance of the global resource. Change to the global resource are visible to other processes that are members of the namespace, but are invisible to other processes.\nNamespace透過把global的系統資源封裝，使得處在同個namespace的processes可以看到所處namespace的資源；如果是在別的namespace的process，就也只能看到自己所處的namespace資源，而看不到其他namespace的資源與processes，透過這樣的方式，不同namespace的processes們看起來就像被「隔離」了！\n不過，這只限於被關在container裡面的process，對於host來說，他可以看到每個container裡面所運行的process，只是，以同一個process來說，從host視角所看見的PID，和container裡面的PID又會是不相同的。\n在筆者撰寫文章的當下(2024年)，Linux總共有八種namespaces，分別是Cgroup, IPC, Network, Mount, PID, Time. User, UTS。簡單來說，每個namespace的種類名稱就代表它隔離了甚麼樣的資源。因為等等我們會需要使用Golang建立namespace，在這邊附上Linux manual table對於各個namespace的介紹，待會實作時我們也會使用到這些namespace：\nNamespace Flag Page Isolates Cgroup CLONE_NEWCGROUP cgroup_namespaces(7) Cgroup root directory IPC CLONE_NEWIPC ipc_namespaces(7) System V IPC, POSIX message queues Network CLONE_NEWNET network_namespaces(7) Network devices, stacks, ports, etc. Mount CLONE_NEWNS mount_namespaces(7) Mount points PID CLONE_NEWPID pid_namespaces(7) Process IDs Time CLONE_NEWTIME time_namespaces(7) Boot and monotonic clocks User CLONE_NEWUSER user_namespaces(7) User and group IDs UTS CLONE_NEWUTS uts_namespaces(7) Hostname and NIS domain name Linux manual page中的namespace介紹 cgroup 現在，我們擁有了多個獨立運行的namespace，cgroup則幫我們實現了namespace之間的資源分配與限制。cgroup的全名叫做Control Group，是Linux Kernel的功能之一，它將processes組織到一個有階層的(hierarchical)組別當中，藉此來監控並限制資源的分配，來對CPU, memory等資源做更精細的控制。\ncgroup把每種可以控制的資源定義為一個子系統(subsystem)，每個子系統都會和kernal的其他模組配合來完成資源控制，例如，CPU子系統會和process scheduling module配合來完成CPU的資源配置。\n所以，以CPU和Memory資源為例，我們的cgroup filesystem架構可能會長得像下面這張圖片：\n圖片取自: https://tech.meituan.com/2015/03/31/cgroups.html 可以看到， Cgroup Hierarchiy A使用了cpu以及cpuacct這兩個subsystem，在Hierarchiy A這個階層中，最上層的/cpu_cgrp作為根節點，可以想像成是它握有這個階層的總資源，並再進一步地將資源分配給它下面的子節點， 因此我們可以說，根節點負責：\n資源分配：因為根節點擁有整個階層的資源，它會按照訂定的策略將資源分配下去。 管理cgroup：根節點會監控每個子節點的資源使用情況，確保資源分配的正確性。 設定屬性與限制：根節點可以設置屬性與限制，而繼承根節點的那些子節點必須遵守。例如，根節點可以設置RAM的使用上限，而這個限制會影響所有繼承它的子節點。 而繼承它的子節點則必須遵守這些配置與限制。因此，子節點(在這裡，就是/cgrp1和/cgrp2)才是這個階層當中的實體cgroup，負責將分配到的資源再分配給process來做使用。\n為了要讓使用者可以自行定義各種資源配置，kernal會把cgroup以檔案系統的形式暴露出來，這個文件系統叫做Virtual File System，它讓使用者可以透過編輯、創建文件，就可以控制process的資源使用。\n大家常提到的cgroup v1和v2的比較，主要也是在檔案系統的管理上的差異。cgroup v1會針對不同的subsystem創建不同的資料夾，如果想要使用不同的subsystem，我們就必須切換到不同的資料夾去建立cgroup；但在v2中，所有的subsystem都會合併到同一個掛載點(通常會是/sys/fs/cgroup)，我們只要進入這個資料夾，就可以看到所有subsystem的配置與控制文件，由此可見，v2相較於v1，簡化了管理配置與管理檔案的流程。\ncgroup v2的檔案系統架構，圖片取自:https://blog.kintone.io/entry/2022/03/08/170206 chroot檔案系統隔離 一般說到container的檔案系統技術，第一時間會想到的應該是Union Filesystem(聯合檔案系統)，不過這次實作並沒有引入這項技術(XD)，所以這邊就不多加贅述，不過非常推薦大家閱讀小賴老師的鐵人賽文章，以原理搭配實驗講解得非常清楚：Day 07: 什麼是 overlay2?\n題外話結束，那麼這個小章節就來聊聊我們要怎麼幫檔案系統做到隔離。可能大家看到這段的第一個想法會是：要做到隔離，namespace不就可以幫我們做到了嗎？但實際上，namespace只是控制裡面的process可以看到甚麼，雖然不同的namespace中的process看不見彼此，但是process還是可以訪問host的檔案系統！若是如此，不僅並沒有完全做到隔離環境，還有可能會導致安全上的問題。\n簡單來說，chroot改變了特定process即其child process的根目錄，接著，process就只能夠訪問新的根目錄以及下面的子目錄，不能夠對這個根目錄之外的檔案進行讀取或者修改，這樣就實現了container與host間的檔案系統隔離了。\n實際上要怎麼去製作container所使用的root directory，我們在下一章節會使用Golang來進行實作。\n小小後記 本章節所提到關於namespace, cgroup, filesystem的技術，因為篇幅有限，加上本次主題旨在如何透過Golang實作container，因此在內容部分僅點到為止，沒有更深入的探討。尤其是cgroup，可以探討的概念、原理與使用方法(例如cgroup怎麼再往下分配process資源，以及實際如何使用VFS來管理cgroup資源，都還來不及講到)，我認為還蠻值得再花一篇文章介紹它的(先挖坑給自己跳)。\nReference Docker: Use containers to Build, Share and Run your applications\nLinux manual page: namespace(7)\nLinux manual page: cgroups(7)\nThe Linux Kernal Documentation: CGROUPS\nFive Things to Prepare for Cgroup v2 with Kubernetes\n那些關於 docker 你知道與不知道的事-Day 10: 什麼是 namespace?\nLinux資源管理之cgroups簡介\nLinux manual page: chroot(2)\n","permalink":"https://sophie0730.github.io/posts/2024/05/build_container_by_go/","summary":"前言 開始第一份工作以後，真切體會到容器化技術的強大與方便之處，工作中處處離不開container，但自己又真的懂它幫我們做了什麼事情嗎？為了更了解容器化技術的底層原理，那不如就自己來做一個container看看好了！\nCNCF的開發專案大多由Golang寫成，同時做為一個語法簡潔、易讀、擁有強大併發處理能力的語言，我相信使用它來建構容器等系統工具是個好選擇，因此本文將會以Golang作為程式碼的範例。\n什麼時候會需要用到container? 要回答這個問題，我們先來看看Docker官方對於container的解釋：\nA container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another.\n一年前我還是個連docker都沒聽過的程式小白，當我想把網站部署到雲端上時，我的作法就是直接把github的程式碼clone到我的機器上，直接加裝任何程式碼所需要的套件及程式語言。現在回想起來，直接在機器上面運行程式碼實在有太多風險了：錯誤的程式碼可能造成系統損壞、資源過度消耗，甚至是運行的程式碼可能包含惡意程式而導致安全漏洞……除此之外，直接在機器上加裝一堆套件也讓環境變得又髒又亂，更別說在多人協作的開發場合中，建置環境時也很常發生「為什麼程式碼在你的電腦可以跑，我的不行？」的惱人狀況。\n基於以上痛點，我們再回頭來看container的定義：container是一個標準的軟體單位，它把程式碼以及所需要的環境與依賴項目給一起打包，讓整個應用程式可以快速地被搬運到另一個運算環境並且可靠地運行。有了container，我們不但可以避免直接運行程式碼的風險，開發環境與生產環境也都會變得乾淨許多。\n那麼，container是怎麼做到的？以上面的定義來看，container做到了環境打包、隔離，這兩個功能對應到的linux技術即是filesystem以及namespace，除此之外，host也需要去管理與限制container可以使用的資源，而這部分就屬於cgroups的範疇。因此，為了更了解container的底層原理，以下將會敘述這三個技術是怎麼成就container的。\nNamespace 當我們運行一個container時，會發現在container當中，我們只能看到在container裡運行的process，如果先前對container有一些認識，大概會知道container是使用命名空間來做到隔離。\n我們直接來看Linux manual page中對於Namespace的解釋：\nA namespace wraps a global system resource in an abstraction thatmakes it appear to the processes within the namespace that theyhave their own isolated instance of the global resource.","title":"動手用Golang實作一個container - 概念篇"},{"content":"前言 最近在學習Golang，在架設http server時，發現Golang在v1.8推出了一個叫做Graceful Shutdown的功能，說來慚愧，先前寫的幾個小專案雖然也都是http server，卻沒有使用過像這樣的功能，所以用一篇小文章來記錄一下。\n什麼是Graceful Shutdown? 先想像一下今天已經有一個已經上線的電商網站服務，使用者會在這個網站上面瀏覽、購買商品，當這個網站要升版時，服務就必須暫停 — — 意思是，所有還在網站上進行的交易、連線都會被中斷。若是像這樣強制關閉服務，待下一次服務啟動時，我們可能就會發現資料上會有預期外的錯誤與差異。\nGraceful Shutdown直譯來說就是「優雅地關機」，這個意思是，當伺服器收到終止的指令後，如果手上還有正在執行的process，它會先處理完，之後才會真的關閉服務，這麼作不僅可以保障資料的一致與完整性，我們也不需要害怕突然中止程式可能會導致非預期的錯誤。\n普通的HTTP server 以下我會先介紹「沒有」Graceful Shutdown機制且強制關閉服務的觀察現象，在這邊，HTTP server的實作會透過Gin framework來實現。\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func main() { log.Println(\u0026#34;starting server...\u0026#34;) router := gin.Default() router.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { time.Sleep(10 * time.Second) c.String(http.StatusOK, \u0026#34;hello there\u0026#34;) }) srv := \u0026amp;http.Server{ Addr: \u0026#34;:8080\u0026#34;, Handler: router, } srv.ListenAndServe() } 根據以上設計，當我們前往localhost:8080後，等十秒會收到從server回傳的字串；若我們在這十秒內強制中止server，client連線也就會跟著被強制中斷，導致終端機噴出以下錯誤：\n* Empty reply from server * Closing connection 0 curl: (52) Empty reply from server Graceful Shutdown in HTTP server 前一次實驗的錯誤是因為當我們結束server服務時，還有一個client連線還沒有收到預期的回覆，卻被強制中斷連線所導致的錯誤。\n而Golang推出的Graceful Shutdown功能，則是當我們結束server服務時，他會先把連接阜給關閉，接著，對於剩下還沒執行完的連線，會等待他們執行完畢後並一一關閉。\n首先我們一樣先定義好server的連接阜以及router。\nlog.Println(\u0026#34;starting server...\u0026#34;) router := gin.Default() router.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ \u0026#34;hello\u0026#34;: \u0026#34;sophie\u0026#34;, }) }) srv := \u0026amp;http.Server{ Addr: \u0026#34;:8080\u0026#34;, Handler: router, } 創建一個在背景運行的goroutine，並在goroutine裡執行server 服務。這麼作是為了不要讓server的運行與後續我們要讓client連線執行完畢的後續流程互相干擾。\ngo func() { if err := srv.ListenAndServe(); err != nil \u0026amp;\u0026amp; err != http.ErrServerClosed { log.Fatalf(\u0026#34;Failed to initialize server: %v\\n\u0026#34;, err) } }() 宣告一個接收os.Signal訊號的channel，如果系統的SIGINT或者SIGTERM訊號被發出，就會關閉quit通道。\n換句話說，如果關閉服務的訊號一直沒有被發出，channel就會一直卡在那邊，以維持正常運作的狀態；當訊號發出後，通道被關閉，後續流程就會被啟動。\n編按：當我們直接kill process時，會收到SIGINT訊號；若這個服務包在docker中，而我們執行docker rm 時，會收到SIGTERM訊號。\nquit := make(chan os.Signal) signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM) \u0026lt;-quit 當關閉訊號發出後，我們傳入一個context，讓程式等待十秒的時間（這邊的等待時間可以根據自己需求作更新），同時，我們將這個context傳入Shutdown函式，讓程式可以在規定的時間內完成所有正在執行的請求，如果規定的時間到了，仍有請求尚未完成，那麼就會強制關閉連線。\n也就是說，假設我們設定的時間不夠久，整個作法就還是強制關閉連線，因此在時間設定上，必須考量到自身server處理資料的時間。\nctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) //確保程式結束後，釋放相關資源 defer cancel() log.Println(\u0026#34;Shutting down server...\u0026#34;) if err := srv.Shutdown(ctx); err != nil { log.Fatalf(\u0026#34;Server forced to shutdown: %v\\n\u0026#34;, err) } 大功告成，整體的程式碼可以參照以下：\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;os/signal\u0026#34; \u0026#34;syscall\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func main() { log.Println(\u0026#34;starting server...\u0026#34;) router := gin.Default() router.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { time.Sleep(10 * time.Second) c.JSON(http.StatusOK, gin.H{ \u0026#34;hello\u0026#34;: \u0026#34;sophie\u0026#34;, }) }) srv := \u0026amp;http.Server{ Addr: \u0026#34;:8080\u0026#34;, Handler: router, } go func() { if err := srv.ListenAndServe(); err != nil \u0026amp;\u0026amp; err != http.ErrServerClosed { log.Fatalf(\u0026#34;Failed to initialize server: %v\\n\u0026#34;, err) } }() log.Printf(\u0026#34;Listening on port %v\\n\u0026#34;, srv.Addr) quit := make(chan os.Signal) signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM) \u0026lt;-quit ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) defer cancel() log.Println(\u0026#34;Shutting down server...\u0026#34;) if err := srv.Shutdown(ctx); err != nil { log.Fatalf(\u0026#34;Server forced to shutdown: %v\\n\u0026#34;, err) } } 使用改版後的程式碼運行後，會發現第一次實驗出現的錯誤消失了，並且成功拿到從server端回傳的字串。\nserver 中途shutdown後，client端還是有拿到server回傳的字串 server端接收到關閉訊號後，有多等待10秒，並處理完來自client端的request Reference Go by Example: Signals https://gobyexample.com/signals\n[Go 教學] 什麼是 graceful shutdown? https://blog.wu-boy.com/2020/02/what-is-graceful-shutdown-in-golang/\nGin Web Framework: Graceful restart or stop https://gin-gonic.com/docs/examples/graceful-restart-or-stop/\n","permalink":"https://sophie0730.github.io/posts/2024/05/go_graceful_shotdown/","summary":"前言 最近在學習Golang，在架設http server時，發現Golang在v1.8推出了一個叫做Graceful Shutdown的功能，說來慚愧，先前寫的幾個小專案雖然也都是http server，卻沒有使用過像這樣的功能，所以用一篇小文章來記錄一下。\n什麼是Graceful Shutdown? 先想像一下今天已經有一個已經上線的電商網站服務，使用者會在這個網站上面瀏覽、購買商品，當這個網站要升版時，服務就必須暫停 — — 意思是，所有還在網站上進行的交易、連線都會被中斷。若是像這樣強制關閉服務，待下一次服務啟動時，我們可能就會發現資料上會有預期外的錯誤與差異。\nGraceful Shutdown直譯來說就是「優雅地關機」，這個意思是，當伺服器收到終止的指令後，如果手上還有正在執行的process，它會先處理完，之後才會真的關閉服務，這麼作不僅可以保障資料的一致與完整性，我們也不需要害怕突然中止程式可能會導致非預期的錯誤。\n普通的HTTP server 以下我會先介紹「沒有」Graceful Shutdown機制且強制關閉服務的觀察現象，在這邊，HTTP server的實作會透過Gin framework來實現。\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func main() { log.Println(\u0026#34;starting server...\u0026#34;) router := gin.Default() router.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { time.Sleep(10 * time.Second) c.String(http.StatusOK, \u0026#34;hello there\u0026#34;) }) srv := \u0026amp;http.Server{ Addr: \u0026#34;:8080\u0026#34;, Handler: router, } srv.ListenAndServe() } 根據以上設計，當我們前往localhost:8080後，等十秒會收到從server回傳的字串；若我們在這十秒內強制中止server，client連線也就會跟著被強制中斷，導致終端機噴出以下錯誤：\n* Empty reply from server * Closing connection 0 curl: (52) Empty reply from server Graceful Shutdown in HTTP server 前一次實驗的錯誤是因為當我們結束server服務時，還有一個client連線還沒有收到預期的回覆，卻被強制中斷連線所導致的錯誤。","title":"Golang中的Graceful Shutdown"},{"content":"iTHOME自2022年舉辦第一場SRE Conference，今年已是第三屆，而這也是我從AppWorks School後端班畢業後參加的第一場技術研討會。做為一個剛從後端領域跨足到SRE的新手來說，此行不僅看到各個公司在導入SRE以及kubernetes的評估與考量之外，透過工作坊的動手做，了解了kubernetes絕對不是僅止於撰寫yaml檔而已。感嘆著這條路的水果然很深之外，更因為還有許多地方可以探索而感到非常興奮。\n此篇文章主要是參加幾場演講下來的速記，因為有些演講的筆記較多，可能會分為兩到三篇來撰寫，同時也會以每場演講作為主題劃分。\nData Architecture and Analysis about OpenTelemetry Observability 講者：蘇揮原 (Mars), TrendMicro\n講者一開始先從趨勢科技的自有產品 - Vision One作為引言，當產品從\u0026quot;Security Tool\u0026quot;逐漸壯大成一個\u0026quot;Cybersecurity Platform\u0026quot;時，那我們該怎麼去管理這些服務？我們可以從下面那張圖看到，Vision One透過單一的平台服務來偵測、預防與應對來自不同地方的資安攻擊與風險，並搭配自動化與人工智慧來落實全方位的資安管理。\n圖片擷取自趨勢科技官方網站：https://www.trendmicro.com/zh_tw/business/products/one-platform.html 那麼，有這麼多的服務都運行在單一的平台上面，勢必得做好管理。講者在這裡提到了兩個名詞：Proactive monitoring以及Observability。我會佔用以下小小的篇幅來大致講述這兩個名詞概念。\n許多針對監控相關的產品網站都提到了proactive monitoring的概念，而我在Datadog官方網站上找到proactive monitoring的定義為： Proactive monitoring is key to flagging potential issues with your applications and infrastructure early, enabling you to respond quickly and reduce downtime.\n意思即是，主動監控是及早發現應用程式與基礎架構潛在問題的關鍵，它幫助我們能快速針對這些問題做出反應，減少server downtime。\n在這裡講者也針對proactive monitoring拋出了一個概念：在用戶發現前先發現問題。\n另一方面，與Proactive Monitoring相互輝映的名詞及是Observebility，以我自己的邏輯來看，我們已經了解到了Proactive Monitoring的好處，那我們該怎麼去做到實際上的監控？第一，我們的系統必須具備可以被觀測(Observable)的能力；再來，透過這些觀測到的資訊，它應該要能幫助我們了解目前系統或者服務的狀態，且我們能有效利用這些資訊來做出適當的判斷。\n在這裡也一併附上CNCF(Cloud Native Computing Foundation)對於Observability的解釋：\nObservability is a system property that defines the degree to which the system can generate actionable insights. It allows users to understand a system’s state from these external outputs and take (corrective) action.\nObservable systems yield meaningful, actionable data to their operators, allowing them to achieve favorable outcomes (faster incident response, increased developer productivity) and less toil and downtime.\nConsequently, how observable a system is will significantly impact its operating and development costs.\nOpenTelemetry Concept \u0026amp; Data Architecture 鋪陳到這裡總算進入到正題了！如同上面提到的，監控即是指我們怎麼去利用手上拿到的資料，做出最適當的判斷，在用戶發現之前提早偵測問題去解決。\nOpenTelemetry（簡稱OTel）是雲原生的可觀測性框架，協助開發者蒐集並導出Observability Signal(Metric, Log, Trace)，他提供標準化的API及SDK來降低蒐集數據的困難度，進而讓開發者們能更方便地進行後續的資料分析以了解系統的性能與行為。\n編按： 為了做到軟體服務的可觀測性(Observebility)，服務必須要能夠發出如Metric, Log, Trace等資料，在這裡，我們稱做這些資料為Observability Signal，OpenTelemetry即是透過蒐集這些資料，把這些資料轉送到後端，進而達成監控的目的。\n既然我們已經了解了OpenTelemetry的功能與目的，那麼就可以把它與其他監控工具如Prometheus, Grafana等串聯在一起，建構完整的Data Architecture。\n在這邊講者有秀出在公司實踐過的系統架構，在這篇筆記就不多加贅述，在這部分其實講者有引入幾個在設計這種data pipeline時可以有的幾個思路：\n當資料量非常龐大的時候，要怎麼做才能有效率地進行資料查詢與分析？　＞　可能需要尋找一個適當的資料倉儲？ Throughtput: 系統要可以掌握大量的資料擷取(data ingestion)與查詢(queries) Analysis: 可以根據不同的情境輕鬆地產出對應的資料分析，甚至這些資料要能餵給LLM進行機器學習 Cost: 成本控制 綜合以上的思路與考量，我們可以選擇OLAP Data Warehouse來有效處理大量資料並做對應的數據呈現，至於為什麼選擇OLAP，請看下個章節的介紹。\nOLAP Data Warehouse \u0026amp; ClickHouse 每種資料庫進行數據儲存的方式不盡相同，不同的儲存方式都有其適合的情境，當資料量一大、系統負載不斷增加時，要選擇何種的儲存方式就顯得非常重要。\nOLAP(Online Analyticla Processing, 線上分析處理)通常用於儲存和處理大量資料，以下簡單列點幾項OLAP的特性以及擅長處理之情境：\n當大多數的請求都是以讀取資料為主時 查詢時間非常快速 可以從資料庫中快速讀取很大筆的行資料 高吞吐量(throughtput) 不太需要資料庫的transaction特性 資料需要可以被篩選或聚合，讓資料庫的查詢結果比來源資料還要小 綜合以上，當資料庫儲存資料的結構上採取Column-Based時，就可以很好地實現或應對以上情境，這是因為資料採取分欄位儲存的方式，當我們只需要提取某些欄位的資料時，我們只需要讀取特定的欄位，而不需要讀取整張資料表。\n{ \u0026#34;id\u0026#34;: [1, 2, 3], \u0026#34;name\u0026#34;: [\u0026#34;John Doe\u0026#34;, \u0026#34;Jane Smith\u0026#34;, \u0026#34;Mike Johnson\u0026#34;], \u0026#34;age\u0026#34;: [30, 25, 28], \u0026#34;email\u0026#34;: [\u0026#34;johndoe@example.com\u0026#34;, \u0026#34;janesmith@example.com\u0026#34;, \u0026#34;mikejohnson@example.com\u0026#34;], \u0026#34;occupation\u0026#34;: [\u0026#34;Software Developer\u0026#34;, \u0026#34;Data Analyst\u0026#34;, \u0026#34;Product Manager\u0026#34;] } column-base data structure 基於OLAP以上的特性，它時常與Data Warehouse結合使用，來增強Data Warehouse的分析能力與效能。而我們可以結合OLAP servers 以及Data Warehouse來完整從資料收集一直到產生儀表板的流程。\n圖片出處：Readings in Database Systems, 3rd Edition　Stonebraker \u0026 Hellerstein, eds. 透過上面的示意圖，我們也可以說從一開始的information sources到Data Warehouse這一段，採用的是ETL(Extract, Transform, Load)，主要著重在技術邏輯(Technical Logic)；而從Dataware House一直到用戶端產生儀表板或者數據分析的這一段，採用的是ELT(Extract, Load, Transform)，這邊我自己的理解是先把資料撈出來，再根據用戶端的商業邏輯與需求，將資料進行轉換並產生適當的圖表，因此相對於前半段，這邊主要是根據商業邏輯(Business Logic)做出的流程。\n用於OLAP情境的資料庫系統非常多，在這場演講中，講者特別介紹了ClickHouse這項產品，因此我稍微搜尋了一下ClickHouse的介紹以及優勢。\nClickHouse® is a high-performance, column-oriented SQL database management system (DBMS) for online analytical processing (OLAP). It is available as both an open-source software and a cloud offering.\nClickHouse 官網介紹 作為一個column-based 的資料庫管理系統，除了上述提到的column-based資料庫應該具備的優勢與特性以外，他也支援OpenTelemetry的exporter - 沒錯，就是前面那個章節提到的雲原生框架；在性能方面，ClickHouse透過分布式查詢、並行(Concurrency)處理、資料壓縮等方式，來完成更有效率的資料庫查詢，同時它也支持基於ANSI SQL standard的查詢語言。\nClickHouse Keeper ClickHouse 資料庫支援分散式與複製資料的功能，其方法是透過將資料切成多個片段(shards)，而這些資料片段則會分布在不同的資料庫節點上，而多個節點則可以集合成一個叢集(cluster)。 ClickHouse的官方網站聲稱\u0026quot;ClickHouse Keeper is a drop-in replacement for ZooKeeper\u0026quot;，在ClickHouse cluster的不同節點之間，它確保集群中的各個節點可以保持一致性且能互相協調工作，且相較於Zookeeper，對於同樣的資料量來說，它在Memory, CPU等效能方面的表現都較Zookeeper還要來得好。\nClickHouse keeper與Zookeeper在memory usage的表現差異 圖片出處：ClickHouse官網 ClickHouse Keeper使用兩種不同的方式來做讀/寫操作。在叢集當中，每個節點上面都會有一個local table，當我們在寫入資料時，則會直接把資料寫入某個節點上面的local table。但當我們要做讀操作時，則會先經過一個叫做Distributed Table的虛擬表，它會把各個節點上面的local table資料給匯集起來，這樣即使我們不個別探訪每個node上面的local table，也能讀取到完整的資料。\nSystem Design (ClickHouse as storage) 綜上所述，ClickHouse協助我們處理需要大量查詢、大量讀取資料的情境，因此，當我們要設計一個Monitoring pipeline時，可以結合前面說到的ETL, ELT的觀念，並將原先收集資料的Prometheus取代成ClickHouse： Kafka + OpenTelemetry (ETL, 使用Kafka處理data streaming並使用OpenTelemetry蒐集資料) -\u0026gt; ClickHouse Cluster(ELT) -\u0026gt; Grafana (呈現資料)\n心得 在AppWorks School時的個人專案剛好就做了一個監控系統，在其中的幾場面試中，也常被問到該如何處理大量寫入、大量讀取的情境。而在實際的業務需求上，常聽到的監控系統不外乎就是Prometheus串聯Grafana來做使用，但當需要監控的資料量非常龐大時，要怎麼提升處理資料的效能便是一件值得團隊來思考的問題。在這場演講中學習了不同的資料處理方式，以及一個團隊在面臨效能瓶頸時，是如何在不斷地調整架構與測試中，找到最適合團隊的設計。\n講者最後還有提到ClickHouse的另一個功能 - Feature Stores，它用來儲存、查詢、管理機器學習的特徵，但在這部分本文章就不花篇幅敘述，可以參考官方ClickHouse的介紹影片：\nReference 趨勢科技 - Trend Vision One https://www.trendmicro.com/zh_tw/business/products/one-platform.html\nOLAP 和 OLTP 有什麼區別？ https://aws.amazon.com/tw/compare/the-difference-between-olap-and-oltp/\nWhat is OLAP and how can you use it in data warehousing? https://www.linkedin.com/advice/0/what-olap-how-can-you-use-data-warehousing-skills-data-management-9fpnc\nClickHouse Document https://clickhouse.com/docs/en/intro\nPowering Feature Stores with ClickHouse https://clickhouse.com/blog/powering-featurestores-with-clickhouse\n踏上 MLOps 之路：從 Applied Data Scientist 到 MLOps 的轉變與建構：Day 18 Feature Store https://ithelp.ithome.com.tw/articles/10324947?sc=rss.iron\n","permalink":"https://sophie0730.github.io/posts/2024/05/sre_conference_1/","summary":"iTHOME自2022年舉辦第一場SRE Conference，今年已是第三屆，而這也是我從AppWorks School後端班畢業後參加的第一場技術研討會。做為一個剛從後端領域跨足到SRE的新手來說，此行不僅看到各個公司在導入SRE以及kubernetes的評估與考量之外，透過工作坊的動手做，了解了kubernetes絕對不是僅止於撰寫yaml檔而已。感嘆著這條路的水果然很深之外，更因為還有許多地方可以探索而感到非常興奮。\n此篇文章主要是參加幾場演講下來的速記，因為有些演講的筆記較多，可能會分為兩到三篇來撰寫，同時也會以每場演講作為主題劃分。\nData Architecture and Analysis about OpenTelemetry Observability 講者：蘇揮原 (Mars), TrendMicro\n講者一開始先從趨勢科技的自有產品 - Vision One作為引言，當產品從\u0026quot;Security Tool\u0026quot;逐漸壯大成一個\u0026quot;Cybersecurity Platform\u0026quot;時，那我們該怎麼去管理這些服務？我們可以從下面那張圖看到，Vision One透過單一的平台服務來偵測、預防與應對來自不同地方的資安攻擊與風險，並搭配自動化與人工智慧來落實全方位的資安管理。\n圖片擷取自趨勢科技官方網站：https://www.trendmicro.com/zh_tw/business/products/one-platform.html 那麼，有這麼多的服務都運行在單一的平台上面，勢必得做好管理。講者在這裡提到了兩個名詞：Proactive monitoring以及Observability。我會佔用以下小小的篇幅來大致講述這兩個名詞概念。\n許多針對監控相關的產品網站都提到了proactive monitoring的概念，而我在Datadog官方網站上找到proactive monitoring的定義為： Proactive monitoring is key to flagging potential issues with your applications and infrastructure early, enabling you to respond quickly and reduce downtime.\n意思即是，主動監控是及早發現應用程式與基礎架構潛在問題的關鍵，它幫助我們能快速針對這些問題做出反應，減少server downtime。\n在這裡講者也針對proactive monitoring拋出了一個概念：在用戶發現前先發現問題。\n另一方面，與Proactive Monitoring相互輝映的名詞及是Observebility，以我自己的邏輯來看，我們已經了解到了Proactive Monitoring的好處，那我們該怎麼去做到實際上的監控？第一，我們的系統必須具備可以被觀測(Observable)的能力；再來，透過這些觀測到的資訊，它應該要能幫助我們了解目前系統或者服務的狀態，且我們能有效利用這些資訊來做出適當的判斷。\n在這裡也一併附上CNCF(Cloud Native Computing Foundation)對於Observability的解釋：\nObservability is a system property that defines the degree to which the system can generate actionable insights.","title":"2024 SRE Conference Record (1)"}]