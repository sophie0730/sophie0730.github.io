[{"content":"因為剛換了一份具有挑戰性的新工作，有好一陣子沒有發文了，雖然手上有幾個在進行的學習或者project，但進度都還沒到可以發表文章的程度。剛好最近工作上有一個之前沒碰過的問題，花了一些時間做處理，因此在這邊簡單地記錄一下。\n問題敘述 在某個上班日的下午，筆者突然收到來自前端工程師的訊息，在跑CI process時GitLab Runner好像出了問題，導致pipeline fail了。經查詢後，發現是該Runner所在的VM disk 容量已達99.9%，已經不能夠再pull任何的image下來了。\nGitLab Runner在執行CICD job時，會pull許多暫時的image來完成任務。在敝公司的每一台Runner中，都有一個固定於每日凌晨執行的cronjob來清除這些job產生出來的image、volume以及build cache等。因此在一開始，筆者懷疑cronjob是否根本沒有如期執行，從而往system log去查看，想確認cronjob執行的狀況。\n不料，cronjob的確有在指定的時間執行。但僅從system log並沒有辦法得知更詳細的資訊——例如執行前的硬體容量多少？執行後多少？如果cronjob確實有做容量的清除，那麼硬碟滿載的情況下大都在一天中的何時發生？SRE該如何即時得知這些訊息？\n可以從以上資訊得知，Gitlab Runner所在的虛擬機缺乏監控設施，導致我們沒有辦法即時、有效地獲得系統狀況，進而做出適合的判斷與解決方案。因此，建立系統的可觀測性便是開始解決問題的第一步。\n建立系統可觀測性：收集系統指標 敝公司的其他專案使用Prometheus + Grafana作為主要監控工具之一，又筆者只需要監控Linux中的系統指標，Prometheus已有現成的exporter可以做使用。綜合考量下，筆者在每一台Runner起了node_exporter的container以收集host-level的metrics，再使用Grafana去收集這些指標，繪製成可視化圖表。\n在這邊提一個小題外話，筆者先前也有自己實作過host-level的metrics exporter，只不過當時單純地認為，如果使用container部署這個服務，那麼觀察的指標不就只限於container裡面了嗎？事實上，在萬物皆檔案(Everything is a file)的Linux當中，我們可以在/proc、/sys等目錄找到CPU、Memory、Disk相關的資訊，因此如果想觀察host的指標，就只需要把host的檔案目錄掛載到container中就可以了！\nnode_exporter的官方GitHub中就給了docker-compose.yaml的最佳範例：\n--- version: \u0026#39;3.8\u0026#39; services: node_exporter: image: quay.io/prometheus/node-exporter:latest container_name: node_exporter command: - \u0026#39;--path.rootfs=/host\u0026#39; network_mode: host pid: host restart: unless-stopped volumes: - \u0026#39;/:/host:ro,rslave\u0026#39; 在監控與告警皆架設完成後（這部分其實還有一些細碎的小東西可以談，例如告警的閾值怎麼設？要監控哪些指標？因為怕偏離重點，先暫且不提），筆者發現，在上班時間，每一台Runner的硬碟佔用量已達全部容量的70~80%，約是400多GB左右，那自然是撐不到在半夜執行的cronjob。此外，即便筆者手動執行完docker system prune -a -f這個可以清除unused container、image以及building cache的指令，也只清除了80G左右的容量。\n基本上，每一台Runner所在的虛擬機就只有裝Runner這個應用程式，而且部署方式還是container，怎麼會佔用快要400GB的容量呢？\n最後，透過資深同事的幫助，以下列指令發現這龐大的體積來源來自於Docker下面的overlay2資料夾。光是這個資料夾就佔用了300多GB。\ndu -ch --max-depth=1 /datadrive/docker/ Docker v25後才修復的bug 難道是連docker system prune都沒有辦法清理overlay嗎？錯了，docker system prune這個指令就是拿來清除任何暫停的container、沒有在使用的network, images, build cache。之所以會沒辦法完全清除，是因為這是Docker v25以前的一個bug（剛好敝公司使用的版本是v24\u0026hellip;）\ndocker system prune WARNING! This will remove: - all stopped containers - all networks not used by at least one container - all dangling images - unused build cache Are you sure you want to continue? [y/N] y 執行docker system prune之前，你會看到的警告 筆者在GitHub Issues中得到的資訊，因為這則留言才順利將問題解決 採取行動 定位問題後，首先採取的步驟當然是將Docker進行升級，基本上確認過change log中沒有什麼比較大的breaking change，在升級過程中並不會碰上什麼問題。\n# Upgrade to the latest version sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin # check docker version and status docker version sudo systemctl status docker 不過，在升級到最新版本v27.3.1之後，執行docker system prune -a -f還是沒辦法把overlay2裡面的build cache給清除！在查詢各個技術論壇的討論後，可能原因有以下：\n有可能是目前Docker底下的檔案目錄不完整，導致Docker engine發生非預期的損壞，在這個推論下，也不推薦直接刪除龐大的docker/overlay2資料夾，因為它屬於整個docker filesystem的一部份，不確定僅刪除這個資料夾會不會造成引擎的損壞。\n我們都知道Docker image是由一層一層的layer所組成，除了能用來區分唯獨層/可寫層(container)，透過共用layer的技術來減少image實際所佔用的硬體空間。我們在Dockerfile撰寫的指令就是一層層的layer，在pull image時，也能看見layer會依序的被pull下來，最後成就一個完整的image。\n但是，當image 在創建時，假設實體硬碟已經滿了，image理所當然就會創建失敗，那麼在硬碟滿了之前、已經pull下來的layer呢？它們還是會存在於overlay2的資料夾當中，只是Docker engine已經沒有辦法追蹤到他們，造成這些沒有被創建完成的layer就無法透過Docker指令進行刪除了。\n因此，筆者總結，一開始應是因為Docker本身的bug導致layer無法被完整刪除，又先前發生過幾次硬碟被填滿的狀況，導致許多沒有被創建完成的孤兒layer被留在overlay2資料夾中，即便Docker升級也無法被正確刪除。\n既然這些孤兒layer已無法被拯救，最好的方法便只剩下重啟Docker engine，來讓Docker重新組織並管理新的檔案系統。\n# DANGER, this will reset docker, deleting all containers, images, and volumes systemctl stop docker systemctl stop docker.socket sudo rm -rf /datadrive/docker systemctl start docker systemctl start docker.socket 重啟Docker以後，會看到Docker檔案系統已經重新長了回來，狀態和功能也都一切正常。之後，在使用docker system prune指令時，就能正確清除overlay2資料夾裡所用不到的layers了。\n後記 這次算是比較幸運，Gitlab Runner本身就是一個stateless的服務，不太需要去規劃資料的備份與保存，如果下次再遇到類似的事情，又是stateful的服務的話，可能就還需要多加一個資料備份的步驟，避免刪除整個Docker檔案系統而發生無法挽回的憾事。\n本篇來不及講到的指標監控和告警設置的部分，也是筆者認為非常有趣的部分，就等到下一篇文章再與各位分享。\n","permalink":"http://localhost:1313/posts/2024/10/note-docker-overlay/","summary":"因為剛換了一份具有挑戰性的新工作，有好一陣子沒有發文了，雖然手上有幾個在進行的學習或者project，但進度都還沒到可以發表文章的程度。剛好最近工作上有一個之前沒碰過的問題，花了一些時間做處理，因此在這邊簡單地記錄一下。\n問題敘述 在某個上班日的下午，筆者突然收到來自前端工程師的訊息，在跑CI process時GitLab Runner好像出了問題，導致pipeline fail了。經查詢後，發現是該Runner所在的VM disk 容量已達99.9%，已經不能夠再pull任何的image下來了。\nGitLab Runner在執行CICD job時，會pull許多暫時的image來完成任務。在敝公司的每一台Runner中，都有一個固定於每日凌晨執行的cronjob來清除這些job產生出來的image、volume以及build cache等。因此在一開始，筆者懷疑cronjob是否根本沒有如期執行，從而往system log去查看，想確認cronjob執行的狀況。\n不料，cronjob的確有在指定的時間執行。但僅從system log並沒有辦法得知更詳細的資訊——例如執行前的硬體容量多少？執行後多少？如果cronjob確實有做容量的清除，那麼硬碟滿載的情況下大都在一天中的何時發生？SRE該如何即時得知這些訊息？\n可以從以上資訊得知，Gitlab Runner所在的虛擬機缺乏監控設施，導致我們沒有辦法即時、有效地獲得系統狀況，進而做出適合的判斷與解決方案。因此，建立系統的可觀測性便是開始解決問題的第一步。\n建立系統可觀測性：收集系統指標 敝公司的其他專案使用Prometheus + Grafana作為主要監控工具之一，又筆者只需要監控Linux中的系統指標，Prometheus已有現成的exporter可以做使用。綜合考量下，筆者在每一台Runner起了node_exporter的container以收集host-level的metrics，再使用Grafana去收集這些指標，繪製成可視化圖表。\n在這邊提一個小題外話，筆者先前也有自己實作過host-level的metrics exporter，只不過當時單純地認為，如果使用container部署這個服務，那麼觀察的指標不就只限於container裡面了嗎？事實上，在萬物皆檔案(Everything is a file)的Linux當中，我們可以在/proc、/sys等目錄找到CPU、Memory、Disk相關的資訊，因此如果想觀察host的指標，就只需要把host的檔案目錄掛載到container中就可以了！\nnode_exporter的官方GitHub中就給了docker-compose.yaml的最佳範例：\n--- version: \u0026#39;3.8\u0026#39; services: node_exporter: image: quay.io/prometheus/node-exporter:latest container_name: node_exporter command: - \u0026#39;--path.rootfs=/host\u0026#39; network_mode: host pid: host restart: unless-stopped volumes: - \u0026#39;/:/host:ro,rslave\u0026#39; 在監控與告警皆架設完成後（這部分其實還有一些細碎的小東西可以談，例如告警的閾值怎麼設？要監控哪些指標？因為怕偏離重點，先暫且不提），筆者發現，在上班時間，每一台Runner的硬碟佔用量已達全部容量的70~80%，約是400多GB左右，那自然是撐不到在半夜執行的cronjob。此外，即便筆者手動執行完docker system prune -a -f這個可以清除unused container、image以及building cache的指令，也只清除了80G左右的容量。\n基本上，每一台Runner所在的虛擬機就只有裝Runner這個應用程式，而且部署方式還是container，怎麼會佔用快要400GB的容量呢？\n最後，透過資深同事的幫助，以下列指令發現這龐大的體積來源來自於Docker下面的overlay2資料夾。光是這個資料夾就佔用了300多GB。\ndu -ch --max-depth=1 /datadrive/docker/ Docker v25後才修復的bug 難道是連docker system prune都沒有辦法清理overlay嗎？錯了，docker system prune這個指令就是拿來清除任何暫停的container、沒有在使用的network, images, build cache。之所以會沒辦法完全清除，是因為這是Docker v25以前的一個bug（剛好敝公司使用的版本是v24\u0026hellip;）","title":"清除冗餘的Docker layers"},{"content":"接下來即將進入的新公司與職位，會需要大量與GitLab CI接觸，為了做好準備，這幾天花了些時間自己跑了一遍CI流程。\nGitLab CI Process 根據GitLab官方文件所述，所有CI/CD任務的執行都必須仰賴gitlab-ci.yml這個文件：\nTo use GitLab CI/CD, you start with a .gitlab-ci.yml file at the root of your project. This file specifies the stages, jobs, and scripts to be executed during your CI/CD pipeline.\n當定義好gitlab-ci.yml後，整個GitLab執行CI/CD的流程就會變成：\n當指定分支的程式碼更新(commit or push or pull request) GitLab根據gitlab-ci.yml的定義，同步平行地觸發job (編按：在沒有任何其他設定之下，job都是平行執行的，除非另外在yml設定stage或者needs參數) GitLab server會去檢查每個job所指名的Runner，並把job分配給適當的Runner執行 Runner執行後的結果(CI Logs)會回傳到GitLab server，顯示於Pipeline上 在本篇文章中，會逐一介紹GitLab中Runner的種類，並且嘗試自定義Runner，並在上面運行單元測試。\nGitLab Runner 當GitLab在運行gitlab-ci.yml中定義的Jobs時，實際上是在GitLab Runner上運作的。在這裡，Runner又能根據作用範圍分作三種類型，分別是：\nShared Runner: 在該GitLab server下的所有group或者project都能夠使用。當我們建立project時，系統會自動產生Shared Runner，可以用來處理無標籤(tag)的任務。 Group Runner: 只有在該group下的所有project可以使用。 Specific Runner: 只有在特定的project下可以被使用。本次會使用Specific Runner作為實驗範例。 Executor 這時，Runner會根據我們選擇的Executor決定要採用何種方式與工作環境來完成CI Job。換言之，一個Runner可以被允許擁有多個executor，讓一個Job能運行在多種環境之中。\nExecutor包含了我們所常用的Shell、Docker等環境，詳細內容可以參照這篇官方文件：Executors\n使用AWS EC2 架設Specific Runner 到這邊才算是進入了本篇文章的正題。我會在同一台AWS EC2上架設兩個Runner，一個Runner負責跑單元測試、另一個Runner則負責build image以及push image到container registry上。為了在CI Pipeline中將專案打包成Docker Image，我也在EC2上另外安裝了Docker。\n環境準備 EC2 (t2.micro, 8G SSD, Ubuntu 24.04 x86) Docker version 27.1.1 Runner Installation \u0026amp; Information Runner的創建與註冊可以參考官方文件：Tutorial: Create, register, and run your own project runner\nRunner Name Tag Executor Image Runner for Testing aws Docker alpine Runner for Image build Shell Null Tag的用途是讓我們在撰寫yml檔時，可以根據tag來將job分配到合適的Runner。如果想要得到在這台機器所有Runner的資訊，預設可以到/etc/gitlab-runner/config.toml進行查看。\n為了讓每次的測試環境不受其他外部因素影響，針對跑測試的Runner我選用Docker Executor，而為了避免Docker in Docker(DinD)的複雜配置，在build image的Runner我則是使用shell executor，並直接在EC2上安裝Docker Engine使用。\n.gitlab-ci.yml 此次的CI Process我切分為三個階段，分別是test(單元測試)、build(建構Docker Image)、publish(將Image推上GitLab Container Registry)\nstages: - test - build - publish 這次要拿來做CI的專案包含兩隻RESTful API，分別是會員的登入與登出功能，主要使用的技術如下：\n後端：Python FastAPI 資料庫：mysql:8.0, redis:7.2.5 根據以上需求，我先將資料庫等服務的相關環境變數(當然，僅供測試使用，與正式環境的環境變數無關)建立成全域的variables\nvariables: MYSQL_DATABASE: auth MYSQL_USER: authuser MYSQL_PASSWORD: mysqltest123 MYSQL_ROOT_PASSWORD: rootpassword DATABASE_URL: mysql+mysqlconnector://authuser:mysqltest123@mysql:3306/auth REDIS_URL: redis://redis:6379 為了能正確運行測試，我在script啟動之前，先將專案的環境、dependencies、MySQL指令先安裝完成，接著將test的結果匯出成artifacts(產物)，這樣我們就能在GitLab server上下載每個pipeline對應的測試結果。\n測試工作的yml檔設定如下：\ntests: stage: test tags: - aws services: - name: mysql:5.7 alias: mysql image: python:3.12-alpine before_script: - apk add --no-cache gcc musl-dev mysql-client - pip install --no-cache-dir -r requirements.txt - pip install --no-cache-dir -r test-requirements.txt - until nc -z -v -w30 mysql 3306; do echo \u0026#34;Waiting for database connection...\u0026#34;; sleep 1; done - echo \u0026#34;MySQL started\u0026#34; - mysql -h mysql -u root -prootpassword -e \u0026#34;CREATE DATABASE IF NOT EXISTS auth;\u0026#34; - mysql -h mysql -u root -prootpassword auth \u0026lt; tests/init_db.sql script: - pytest \u0026gt; test-report.txt artifacts: paths: - test-report.txt expire_in: \u0026#34;30 days\u0026#34; 測試通過以後，會進入Image的建構與發佈流程，在這邊比較值得一提的是管理Docker Image名稱的方式。如果每次Image的版本做更動，我們就要到yml檔裡面去修改版號，這樣不僅麻煩，也很容易會有人為錯誤發生。所幸gitlab提供了許多已定義好的CI/CD變數，我們可以在yml檔直接使用這些變數來進行Image名稱的管理。在這裡，我使用CI_REGISTRY_IMAGE作為Image名稱，CI_PIPELINE_IID作為Image的版本號。\n綜合以上，最終的yml檔會是這樣：\nstages: - test - build - publish variables: MYSQL_DATABASE: auth MYSQL_USER: authuser MYSQL_PASSWORD: mysqltest123 MYSQL_ROOT_PASSWORD: rootpassword DATABASE_URL: mysql+mysqlconnector://authuser:mysqltest123@mysql:3306/auth REDIS_URL: redis://redis:6379 IMAGE_VERSION: ${CI_PIPELINE_IID} tests: stage: test tags: - aws services: - name: mysql:5.7 alias: mysql image: python:3.12-alpine before_script: - echo \u0026#34;DATABASE_URL is set to:$DATABASE_URL\u0026#34; - apk add --no-cache gcc musl-dev mysql-client - pip install --no-cache-dir -r requirements.txt - pip install --no-cache-dir -r test-requirements.txt - until nc -z -v -w30 mysql 3306; do echo \u0026#34;Waiting for database connection...\u0026#34;; sleep 1; done - echo \u0026#34;MySQL started\u0026#34; - mysql -h mysql -u root -prootpassword -e \u0026#34;CREATE DATABASE IF NOT EXISTS auth;\u0026#34; - mysql -h mysql -u root -prootpassword auth \u0026lt; tests/init_db.sql script: - pytest \u0026gt; test-report.txt artifacts: paths: - test-report.txt expire_in: \u0026#34;30 days\u0026#34; build: stage: build tags: - build before_script: - echo \u0026#34;Building Docker image\u0026#34; - docker info script: - docker build -t $CI_REGISTRY_IMAGE:$IMAGE_VERSION . - docker tag $CI_REGISTRY_IMAGE:$IMAGE_VERSION $CI_REGISTRY_IMAGE:latest - docker images only: - main publish-to-registry: stage: publish tags: - build before_script: - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY script: - docker push $CI_REGISTRY_IMAGE:$IMAGE_VERSION - docker push $CI_REGISTRY_IMAGE:latest only: - main Trigger CI Pipeline 以目前的設置來說，只要在mainbranch上進行程式碼的更動，三個stage會一起被觸發，否則，就只會觸發test stage。為了執行簡單的測試，我直接在main branch 進行commit，前往Build-\u0026gt;Pipelines就可以看到Pipeline被成功觸發了： 接著，前往Deploy-\u0026gt;Container Registry也能看到方才Pipeline產生的Image被成功上傳，可以看到最新Image的版本號與Pipeline的ID一致，且最新產生的Image有被加上latest標籤。到這邊，一個完整的CI流程就結束了。\n坑：403 forbidden 註冊完gitlab runner並啟動後，console出現以下錯誤：\nERROR: Checking for jobs... forbidden runner=CsjMisCJ- status=POST https://gitlab.com/api/v4/jobs/request: 403 Forbidden 後來發現是因為我先前有註冊過標籤一模一樣的Runner，而這個Runner在Gitlab server中已經被刪除了，我卻忘記在EC2上把該Runner給取消註冊，又由於這兩個Runner的標籤相同，導致Gitlab在分配Job時出現錯誤。\n解決方法當然就是每次要記得把不需要的Runner給unregistered：\n# 取得當前機器上的Runner以及對應的executor、token sudo gitlab-runner list # 根據以上指令獲得的Runner, token, URL來做刪除 sudo gitlab-runner verify --delete -t [token] -u [URL] Reference GitLab Docs https://docs.gitlab.com/\ngitlab-runner run-single always fails with \u0026lsquo;forbidden\u0026rsquo; with docker executor https://gitlab.com/gitlab-org/gitlab-runner/-/issues/4919\nHow do I delete/unregister a GitLab runner https://stackoverflow.com/questions/66616014/how-do-i-delete-unregister-a-gitlab-runner\n","permalink":"http://localhost:1313/posts/2024/07/gitlab-ci/","summary":"接下來即將進入的新公司與職位，會需要大量與GitLab CI接觸，為了做好準備，這幾天花了些時間自己跑了一遍CI流程。\nGitLab CI Process 根據GitLab官方文件所述，所有CI/CD任務的執行都必須仰賴gitlab-ci.yml這個文件：\nTo use GitLab CI/CD, you start with a .gitlab-ci.yml file at the root of your project. This file specifies the stages, jobs, and scripts to be executed during your CI/CD pipeline.\n當定義好gitlab-ci.yml後，整個GitLab執行CI/CD的流程就會變成：\n當指定分支的程式碼更新(commit or push or pull request) GitLab根據gitlab-ci.yml的定義，同步平行地觸發job (編按：在沒有任何其他設定之下，job都是平行執行的，除非另外在yml設定stage或者needs參數) GitLab server會去檢查每個job所指名的Runner，並把job分配給適當的Runner執行 Runner執行後的結果(CI Logs)會回傳到GitLab server，顯示於Pipeline上 在本篇文章中，會逐一介紹GitLab中Runner的種類，並且嘗試自定義Runner，並在上面運行單元測試。\nGitLab Runner 當GitLab在運行gitlab-ci.yml中定義的Jobs時，實際上是在GitLab Runner上運作的。在這裡，Runner又能根據作用範圍分作三種類型，分別是：\nShared Runner: 在該GitLab server下的所有group或者project都能夠使用。當我們建立project時，系統會自動產生Shared Runner，可以用來處理無標籤(tag)的任務。 Group Runner: 只有在該group下的所有project可以使用。 Specific Runner: 只有在特定的project下可以被使用。本次會使用Specific Runner作為實驗範例。 Executor 這時，Runner會根據我們選擇的Executor決定要採用何種方式與工作環境來完成CI Job。換言之，一個Runner可以被允許擁有多個executor，讓一個Job能運行在多種環境之中。","title":"自架GitLab Runner來執行單元測試"},{"content":"前陣子發現一直有在關注的部落格作者，會在自己的部落格分享每個月的學習回顧，覺得這是很好的自省方式，所以也想要跟著做。\n本月重點 這個月主要專注準備AWS SAA-C03證照，中間有跑去CNTUG申請LAB來試著架架看Kubernetes，透過Helm架設Prometheus+Grafana來監控cluster上面運行的服務，也有寫一點Python，刷leetcode防止失智。\n必須要檢討的是，這個月除了證照以外，並沒有訂定明確的讀書計畫，可能也跟我這個月其實很迷惘，到底要往開發還是往SRE走有關，就變成甚麼都碰一點。所以整個六月只有考到證照這件事算是有收穫。\nAWS Solutions Architect - Associate 這張證照總共花費三周時間進行準備。我參考的是Udemy上的知名課程 Ultimate AWS Certified Solutions Architect Associate SAA-C03 搭配EXAMTOPICS的考古題做準備。\n大約花了一周時間把Udemy課程上完，第二周開始邊刷題邊檢討，第三周開始模擬考試情境，每次連續寫65題並計時、算分，再仔細檢討題目，並且與之前學習到的觀念做對照與補充。\n在筆記部份我使用Heptabase，這個筆記軟體之前就有聽聞，但感覺操作起來蠻複雜的就一直沒有想要使用，直到看到AppWorks School同學的推坑文，實際使用之後也算滿意，非常適合準備SAA這種知識比較零散，但解題時又需要組織這些零散知識的考試。\n體感上來說，SAA與其說是考你各個服務的特性，不如比較像是考你「要如何貼合客戶需求，運用AWS服務設計架構，解決當前問題」，因此我覺得雖然很多人說刷題就好，但寫題目看答案的時候還是要問自己「為什麼這個正確答案要這樣設計？」因為實際進考場後真正有寫到重複的考古題並不多，但觀念其實都是類似的。\n儘管考過證照，但自認還是缺乏實作上的經驗，剛好下個月會有一些實做專案的機會，如果把專案丟到AWS上或許能有更多的收穫。\n下月重點 其實以時間點來看已經是這個月了\u0026hellip;因為面試都集中在七月前半段，因此安排如下\n七月中前：\n面試作業，準備面試 資料結構演算法(Binary Tree, Graphs, Backtracking, DP) 七月中後：\nmissing semester of your CS education(看完) 清大計算機網路概論(有開始就好) 每日進度細項 六月份共有6天沒有讀書。\n6/1 參加Docker Workshop，回家後整理筆記\n6/2 使用Golang實作簡單的container，並發表部落格文章\n6/3 Golang web server架設+撰寫部落格文章\n6/4 Golang web server practice\n6/5 AWS EC2\n6/6 AWS ELB \u0026amp; ASG\n6/7 端午節家族旅遊\n6/8 端午節家族旅遊\n6/9 端午節家族旅遊\n6/10 端午節家族旅遊\n6/11 AWS RDS, Aurora, ElastiCache, Route 53, Classic Architecture\n6/12 S3, Advanced S3, S3 Security, CloudFront, Global accelerator, storage extras\n6/13 Decoupling application(SQS, SNS, Kinesis, Amazon MQ), Container\n6/14 serverless, database\n6/15 早上準備面試+下午面試+晚上檢討\n6/16 data analysis, machine learning\n6/17 monitoring, IAM, Security, Disaster recovering, more solution architecture\n6/18 Other services, white paper, prepare exam, networking\n6/19 EXAMTOPIC 1~50題作答+檢討\n6/20 EXAMTOPIC 51~100作答+檢討\n6/21 使用OpenStack搭建kubernetes+撰寫文章\n6/22 參加104就業博覽會\n6/23 休息\n6/24 linkedlist, Udemy practice 1 作答+檢討\n6/25 Python FastAPI practice, 架設監控kubernetes的prometheus+grafana\n6/26 Udemy practice 2 作答+檢討, stack+queue, 架設監控kubernetes的prometheus+grafana並觀察\n6/27 重新複習先前寫過的考古題\n6/28 EXAMTOPIC 500~565\n6/29 EXAMTOPIC 565~629\n6/30 EXAMTOPIC 630~669+總複習\n","permalink":"http://localhost:1313/posts/2024/07/2024_6_review/","summary":"前陣子發現一直有在關注的部落格作者，會在自己的部落格分享每個月的學習回顧，覺得這是很好的自省方式，所以也想要跟著做。\n本月重點 這個月主要專注準備AWS SAA-C03證照，中間有跑去CNTUG申請LAB來試著架架看Kubernetes，透過Helm架設Prometheus+Grafana來監控cluster上面運行的服務，也有寫一點Python，刷leetcode防止失智。\n必須要檢討的是，這個月除了證照以外，並沒有訂定明確的讀書計畫，可能也跟我這個月其實很迷惘，到底要往開發還是往SRE走有關，就變成甚麼都碰一點。所以整個六月只有考到證照這件事算是有收穫。\nAWS Solutions Architect - Associate 這張證照總共花費三周時間進行準備。我參考的是Udemy上的知名課程 Ultimate AWS Certified Solutions Architect Associate SAA-C03 搭配EXAMTOPICS的考古題做準備。\n大約花了一周時間把Udemy課程上完，第二周開始邊刷題邊檢討，第三周開始模擬考試情境，每次連續寫65題並計時、算分，再仔細檢討題目，並且與之前學習到的觀念做對照與補充。\n在筆記部份我使用Heptabase，這個筆記軟體之前就有聽聞，但感覺操作起來蠻複雜的就一直沒有想要使用，直到看到AppWorks School同學的推坑文，實際使用之後也算滿意，非常適合準備SAA這種知識比較零散，但解題時又需要組織這些零散知識的考試。\n體感上來說，SAA與其說是考你各個服務的特性，不如比較像是考你「要如何貼合客戶需求，運用AWS服務設計架構，解決當前問題」，因此我覺得雖然很多人說刷題就好，但寫題目看答案的時候還是要問自己「為什麼這個正確答案要這樣設計？」因為實際進考場後真正有寫到重複的考古題並不多，但觀念其實都是類似的。\n儘管考過證照，但自認還是缺乏實作上的經驗，剛好下個月會有一些實做專案的機會，如果把專案丟到AWS上或許能有更多的收穫。\n下月重點 其實以時間點來看已經是這個月了\u0026hellip;因為面試都集中在七月前半段，因此安排如下\n七月中前：\n面試作業，準備面試 資料結構演算法(Binary Tree, Graphs, Backtracking, DP) 七月中後：\nmissing semester of your CS education(看完) 清大計算機網路概論(有開始就好) 每日進度細項 六月份共有6天沒有讀書。\n6/1 參加Docker Workshop，回家後整理筆記\n6/2 使用Golang實作簡單的container，並發表部落格文章\n6/3 Golang web server架設+撰寫部落格文章\n6/4 Golang web server practice\n6/5 AWS EC2\n6/6 AWS ELB \u0026amp; ASG\n6/7 端午節家族旅遊\n6/8 端午節家族旅遊\n6/9 端午節家族旅遊","title":"2024 06月份讀書狀態回顧"},{"content":"先前參加SRE Conference時，認識了CNTUG(Cloud Native Taiwan User Group)這個開源社群，除了推廣雲端原生的相關技術以外，也提供Lab讓大家能夠申請、在上面做一些很難在自己本機上面的實驗(例如kubernetes cluster的建立)，很幸運地前陣子遞交的Lab申請通過了，就也打算來寫一篇文章記錄整個實驗的架設與心得。\n環境準備 參考了CNTUG網站與kubernetes官方網站上面關於VM硬體條件的文件，這次在openstack上架了四台VM，一台bastion host、一台control plane(m0)、兩台worker node(n0, n1)。 四台VM分別的硬體條件如下：\nIP Address Server Name Role CPU Ram OS 192.168.200.100 bastion-host Bastion Host 2 2G Ubuntu 22.04 192.168.200.101 k8s-m0 Master Node - 0 4 4G Ubuntu 22.04 192.168.200.102 k8s-n0 Worker Node - 0 4 4G Ubuntu 22.04 192.168.200.103 k8s-n1 Worker Node - 1 4 4G Ubuntu 22.04 網路設定 準備兩張網卡：public與private。本實驗環境會將kubernetes cluster都放在內網，僅讓bastion host做對外的連線。\n在拿到openstack的帳號時，public網卡已經先幫我們建立好了，接下來要自己手動新增內網，並且能讓內網去連接到外網。在設定內網時，記得勾選「啟用DHCP」讓每個加進這個網路的VM都會被自動分配到唯一的ip位址。\nPublic Network: Public IPv4: 103.122.XXX.0/23 Private Network: 子網路名稱：private-net 網路位址：192.168.200.0/24 IP版本：IPv4 閘道IP(Gateway IP)：192.168.200.1 靜態路由(內網所有的設備要訪問任何不在內網的位址時，都把封包送到下一跳點的位址): 目標CIDR:0.0.0.0/0 下一跳點：103.122.117.XXX (Public gateway IP) 安全性群組設定 方才我們透過網路來設定各個網路設備的連接，那麼接下來進一步地透過安全性群組來管理端口與每台VM的流量。\n除了kubernetes本身需要開啟的端口以外，我會希望可以透過bastion host來ssh到cluster裡面的每個node，在內網的所有intance也需要互相連結。除此之外，bastion host也要開一個對外的22 port讓我可以從本地進行操作。\nControl Plane和Worker Node的port全部都參考官網建議去設置：kubernetes: Ports and Protocols\n最後，會有四個安全性群組：(所有的outbound都設置為\t0.0.0.0/0，故以下僅列出inbound rule)\nSecurity Group Ether Type IP protocol Port Range Remote IP Prefix Remote Security Group Description Public SSH IPv4 TCP 22 (SSH) (自己的電腦IP) NULL NULL Private SSH IPv4 TCP 22 (SSH) 192.168.200.0/24 NULL NULL Master node IPv4 TCP 2379 - 2380 192.168.200.0/24 etcd server client API IPv4 TCP 6443 192.168.200.0/24 Kubernetes API server IPv4 TCP 10250 NULL Master node Kubelet API IPv4 TCP 10257 NULL Master node kube-controller-manager IPv4 TCP 10259 NULL Master node kube-schedule Worker node IPv4 TCP 10250 NULL Worker node Kubelet API Self Worker node IPv4 TCP 10256 NULL Worker node kube-proxy IPv4 TCP 30000 - 32767 0.0.0.0/0 Worker node NodePort Services Master Node中的etcd元件是一個分散式的key-value資料庫，會保存kubernetes cluster裡面的所有資料，cluster裡面的所有狀態——例如Pod, Volume, Service的當前狀態——都會保存在這個資料庫中，由於在Worker Node的狀態會被回傳到etcd元件，因此開放讓內網裡所有的IP都能連線。\n在Master Node中另一個會需要開放給內網所有instance的原件還有Kubernetes API server，它讓使用者可以直接與cluster進行互動，包含查詢狀態、建立、更新、刪除cluster內部的資源等。同時由於我們先前已讓bastion host來處理所有來自外部的連線，因此在這邊的IP也是設定為內網IP即可。\nVM設置 網路與安全性群組都設定好後，cluster裡面的VM全都插入private網卡，再根據master/worker node分別分派各自的安全性群組。bastion host則插入public與private兩張網卡，以便與外部和內部網路進行連接。\nopenstack的網路設定有個小坑：bastion-host加入private網卡後，依然沒有辦法ping到內網的任何一台VM。經由ip addr確認後證實雖然openstack的UI介面顯示已經加入網卡了，但實際上private網卡並沒有設定到。\n這時候可以手動修改設定檔：\nsudo vim /etc/netplan/50-cloud-init.yaml 在設定檔中手動加入網卡：\nnetwork: ethernets: enp3s0: dhcp4: true match: macaddress: fa:16:3e:4f:f7:6c set-name: enp3s0 enp3s1: # 新增的配置 dhcp4: true match: macaddress: fa:16:3e:1f:77:9f set-name: enp3s1 # 指定新的網卡名稱 version: 2 （以上做法是看tico大大的部落格解決的:https://ithelp.ithome.com.tw/articles/10293768）\nKey generation 我們希望可以從本地電腦連線到bastion-host，也希望bastion-host可以連線到cluster裡面的任一個node。\n先在本地產一個key，把public key上傳到openstack的密鑰對。接著在設置VM時都加入這個密鑰對 把本地的private key用ssh的方式傳到bastion host ssh -i \u0026lt;PRIVATE_KEY_PATH\u0026gt; ubuntu@\u0026lt;PUBLIC_IP\u0026gt; 到這邊，確定四台VM的網路設置沒問題後，才真正可以來架設kubernetes cluster了。\nkubespray kubespray是一個開源專案，用來進行kubernetes cluster的創建。自version 2.3開始，kubespray的內部採用與kubeadm相同的生命週期管理，再透過Ansible Playbook來調用kubeadm來做叢集的建置。 kubeadm則是由官方開發維護，用來建立原生kubernetes環境的工具。因此，使用kubespray不僅兼顧了部署的方便性，也兼顧了kubernetes生命週期管理的穩定性。\n因此在安裝kubespray之前，要先在機器上面安裝Ansible以及Ansible所需要的Python環境。相關安裝步驟我是參考kubespray官方文件：Installing Ansible\n版本資訊 Python3 v3.10.4 Ansible v9.5.1 kubespray v2.25.0 安裝步驟 以下的安裝步驟，若無特別標註，就都是在bastion host來做執行。\n由於從openstack拿到的VM裡面已有內建Python，在這邊可以先裝Python的虛擬環境。\nubuntu@bastion-host:~$ sudo apt update ubuntu@bastion-host:~$ sudo apt install python3-virtualenv ubuntu@bastion-host:~$ virtualenv --version virtualenv 20.13.0+ds from /usr/lib/python3/dist-packages/virtualenv/__init__.py 接下來就可以把kubespray專案clone下來，這邊我裝的版本是撰寫當下的最新版本2.25.0。\ngit clone --depth 1 --branch v2.25.0 https://github.com/kubernetes-sigs/kubespray.git 安裝Ansible\nVENVDIR=kubespray-venv #指定virtual env位置 KUBESPRAYDIR=kubespray # kubespray資料夾位置 python3 -m venv $VENVDIR source $VENVDIR/bin/activate cd $KUBESPRAYDIR pip install -U -r requirements.txt # 安裝Ansible所需套件 如果順利的話，到這邊就可以看到(kubespray-env)已經被啟動了！\n在這邊我們會先把cluster的配置文件拷貝出來，創建一個自己的叢集目錄，方便配置與管理。\ncp -rfp inventory/sample inventory/mycluster 接著，進入到mycluster目錄中，編輯inventory.ini。裡面的設定檔就是我們server相關資訊的設定，其餘的參數代表意義可以參考kubespray的官方文件。\n# ## Configure \u0026#39;ip\u0026#39; variable to bind kubernetes services on a # ## different ip than the default iface # ## We should set etcd_member_name for etcd cluster. The node that is not a etcd member do not need to set the value, or can set the empty string value. [all] # 根據文件範例設定，server_name ansiblehost=\u0026lt;IP\u0026gt; ansible_user=\u0026lt;USERNAME\u0026gt; k8s-m0 ansible_host=192.168.200.101 ansible_user=ubuntu k8s-n0 ansible_host=192.168.200.102 ansible_user=ubuntu k8s-n1 ansible_host=192.168.200.103 ansible_user=ubuntu [kube_control_plane] k8s-m0 [etcd] # etcd的位置，在這邊就是與master node是在同一個位置 k8s-m0 [kube_node] k8s-n0 k8s-n1 [calico_rr] [k8s_cluster:children] kube_control_plane kube_node calico_rr 目前搭建的實驗環境資源有限，因此也將etcd放在master node上。master node目前也沒有做到HA的設計，是以單節點的方式存在。\n也由於master node以單節點的方式存在，不需要Load Balancer API server，我們進到inventory/mycluster/group_vars/all/all.yml把Line 20的loadbalancer_apiserver_localhost修改為False。\n接著，再進到inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml把Line 160 的cluster_name修改成自己的cluster name。\n最後啟動ansible進行部署。\n# ansible-playbook -i \u0026lt;INVENTORY_FILE\u0026gt; --private-key=\u0026lt;PRIVATE_KEY\u0026gt; --become --become-user=root cluster.yml ansible-playbook -i inventory/mycluster/inventory.ini --private-key=~/.ssh/private.key --become --become-user=root cluster.yml 這邊的private key就是在前面文章中，我們要從bastion host連線到cluster node的那把private key。\n在bastion host完成叢集的創建後，日後想要存取叢集需要拿取master node上面的token才行，因此我們先從bastion host SSH到master node。\nssh -i ~/.ssh/private.key ubuntu@192.168.200.101 先前提到kubespray是透過ansible做自動化部署，但底層仍舊遵循kubeadm的lifecycle。/etc/kubernetes/admin.conf在kubeadm初始化叢集時便會創建，可以算是使用者與Kubernetes API Server進行溝通的token。\n我們在操作叢集資源時一般都是使用kubectl來做到，為了要讓kubectl可以使用這個token來做資源操作，我們要修改一下權限，並且把token丟回bastion host，這樣在bastion host也可以使用kubectl 操作資源。\nubuntu@k8s-m0:~$ sudo cp /etc/kubernetes/admin.conf ~/ ubuntu@k8s-m0:~$ sudo chown ubuntu:ubuntu ~/admin.conf # 將user/group修改成當前用戶，這樣不需要sudo 也能操作token ubuntu@k8s-m0:~$ mkdir -p .kube #kubectl 會從這個目錄裡面來讀取相關的config文件 ubuntu@k8s-m0:~$ mv ~/admin.conf ~/.kube/config #將原本kubeadm創建的token移到能被kubectl 讀取的地方 確定能讀取到cluster的訊息後，再把~/.kube/config scp到bastion host的相同目錄下，讓kubectl可以讀取。\n(kubespray-venv) ubuntu@bastion-host:~$ mkdir -p ~/.kube (kubespray-venv) ubuntu@bastion-host:~$ scp -i ~/private.key ubuntu@192.168.200.101:~/.kube/config ~/.kube/config 另外要記得的是，config檔的line 5記錄著API Server的位置，因為我們是直接從master node將文件複製到bastion host，在這邊要記得把127.0.0.1改成192.168.200.101也就是master node的內網IP。\n安裝kubectl 這邊要注意的是kubectl的版本要和cluster的版本相同(可以差到一個minor version)。 查看完cluster版本後就直接按普通方式安裝即可。\n# 從kubectl get node就能查看cluster版本為v1.29.5 kubectl get node NAME STATUS ROLES AGE VERSION k8s-m0 Ready control-plane 24h v1.29.5 k8s-n0 Ready \u0026lt;none\u0026gt; 24h v1.29.5 k8s-n1 Ready \u0026lt;none\u0026gt; 24h v1.29.5 # download a specific version of kubectl curl -LO \u0026#34;https://dl.k8s.io/release/v1.29.5/bin/linux/amd64/kubectl\u0026#34; # Download the kubectl checksum file: curl -LO \u0026#34;https://dl.k8s.io/v1.29.5/bin/linux/amd64/kubectl.sha256\u0026#34; # validate the binary file echo \u0026#34;$(cat kubectl.sha256) kubectl\u0026#34; | sha256sum --check # check ok後就可以正式安裝 sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl # 安裝後，不需要的檔案可以刪除 rm kubectl* 在bastion host下kubectl get node指令，大功告成！ 踩坑紀錄 前面才剛提供一個private網卡踩坑的解決辦法，結果隔天上bastion host就突然沒辦法讀取cluster的資源了。一查看發現是private那張網卡的設定又跑掉了。\nubuntu@bastion-host:~$ ip addr show enp3s1 3: enp3s1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1442 qdisc fq_codel state UP group default qlen 1000 link/ether fa:16:3e:1f:77:9f brd ff:ff:ff:ff:ff:ff inet6 fe80::f816:3eff:fe1f:779f/64 scope link valid_lft forever preferred_lft forever 由上面資訊可以發現它的IPv4配置不見了。因此我又手動加了回去。\nubuntu@bastion-host:~$ sudo ip addr add 192.168.200.100/24 dev enp3s1 ubuntu@bastion-host:~$ ip addr show enp3s1 3: enp3s1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1442 qdisc fq_codel state UP group default qlen 1000 link/ether fa:16:3e:1f:77:9f brd ff:ff:ff:ff:ff:ff inet 192.168.200.100/24 scope global enp3s1 valid_lft forever preferred_lft forever inet6 fe80::f816:3eff:fe1f:779f/64 scope link valid_lft forever preferred_lft forever 成功！但不確定是不是openstack的網路設置本就有一些問題，先把這個坑紀錄，之後看看會不會再發生。\nubuntu@bastion-host:~$ ping 192.168.200.101 PING 192.168.200.101 (192.168.200.101) 56(84) bytes of data. 64 bytes from 192.168.200.101: icmp_seq=1 ttl=64 time=5.44 ms 64 bytes from 192.168.200.101: icmp_seq=2 ttl=64 time=1.41 ms 64 bytes from 192.168.200.101: icmp_seq=3 ttl=64 time=1.06 ms 64 bytes from 192.168.200.101: icmp_seq=4 ttl=64 time=0.696 ms Reference Installing kubeadm https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#before-you-begin\n關於我怎麼把一年內學到的新手 IT/SRE 濃縮到 30 天筆記這檔事 https://ithelp.ithome.com.tw/users/20112934/ironman/5640\nComparison - Kubespray vs Kubeadm https://github.com/kubernetes-sigs/kubespray/blob/master/docs/getting_started/comparisons.md\nInstalling Ansible https://github.com/kubernetes-sigs/kubespray/blob/v2.19.1/docs/ansible.md#inventory\nInstall and Set Up kubectl on Linux https://kubernetes.io/docs/tasks/tools/install-kubectl-linux/\nCNTUG Infra Labs 說明文件 https://docs.cloudnative.tw/docs/category/%E5%9F%BA%E7%A4%8E%E6%95%99%E5%AD%B8\nOpenStack 上利用 kubeadm 搭建 K8S Cluster https://docs.cloudnative.tw/docs/self-paced-labs/kubeadm/\n","permalink":"http://localhost:1313/posts/2024/06/build_a_cluster_by_kubespray/","summary":"先前參加SRE Conference時，認識了CNTUG(Cloud Native Taiwan User Group)這個開源社群，除了推廣雲端原生的相關技術以外，也提供Lab讓大家能夠申請、在上面做一些很難在自己本機上面的實驗(例如kubernetes cluster的建立)，很幸運地前陣子遞交的Lab申請通過了，就也打算來寫一篇文章記錄整個實驗的架設與心得。\n環境準備 參考了CNTUG網站與kubernetes官方網站上面關於VM硬體條件的文件，這次在openstack上架了四台VM，一台bastion host、一台control plane(m0)、兩台worker node(n0, n1)。 四台VM分別的硬體條件如下：\nIP Address Server Name Role CPU Ram OS 192.168.200.100 bastion-host Bastion Host 2 2G Ubuntu 22.04 192.168.200.101 k8s-m0 Master Node - 0 4 4G Ubuntu 22.04 192.168.200.102 k8s-n0 Worker Node - 0 4 4G Ubuntu 22.04 192.168.200.103 k8s-n1 Worker Node - 1 4 4G Ubuntu 22.04 網路設定 準備兩張網卡：public與private。本實驗環境會將kubernetes cluster都放在內網，僅讓bastion host做對外的連線。\n在拿到openstack的帳號時，public網卡已經先幫我們建立好了，接下來要自己手動新增內網，並且能讓內網去連接到外網。在設定內網時，記得勾選「啟用DHCP」讓每個加進這個網路的VM都會被自動分配到唯一的ip位址。\nPublic Network: Public IPv4: 103.122.XXX.0/23 Private Network: 子網路名稱：private-net 網路位址：192.","title":"使用kubespray在OpenStack上搭建kubernetes cluster"},{"content":"來實作一個container吧 延續上篇的動手用Golang實作一個container - 概念篇，了解container的底層技術是如何實踐之後，我們就可以開始使用Golang來做出屬於我們自己的container了。\n以Docker為例，當我們要啟動一個container的時候，會使用這個指令：\ndocker container run \u0026lt;image-name\u0026gt; [cmd] 以此為發想點，當我想要使用我的程式碼啟動container時，他長得會像這樣子：\ngo run main.go run [cmd] 在這裡，我們會分別定義兩種function：\nrun() : parent process要執行的function。負責創建child process及並配置其運行的環境（如namespace）。 child() : child process要執行的function。負責管理在container環境中，要如何運行用戶端所指定的命令。 而must()function則會作為error handler使用。\nfunc main() { switch os.Args[1] { case \u0026#34;run\u0026#34;: run() case \u0026#34;child\u0026#34;: child(); default: panic(\u0026#34;what??\u0026#34;) } } func must(err error) { if err != nil { panic(err) } } 這邊還蠻想提一下os.Args這個指令。os.Args 是Golang裡面用來儲存命令行參數的一個變數。如果我們只單印出os.Args，他會長的像這樣：\nsophie@Sophie-Desktop:~/go-container$ go run test.go run echo \u0026#39;hi\u0026#39; [/tmp/go-build3547203082/b001/exe/test run echo hi] 第一行是我執行Go程式碼的指令，第二行是os.Args印出來的結果。到這裡會發現為什麼go run test.go 不但沒有被儲存在os.Args中，反而還多了一串類似像檔案路徑的東西？\n這是因為當我們使用go run來運行程式時，他會先編譯我們指定的程式碼檔案，創建一個臨時的執行檔，它實際運行的其實是這個臨時創建出來的執行檔。\n因此，這也能說明為什麼go run test.go直接被這個臨時執行檔的路徑給取代了。基於這個架構，我們使用os.Args[1]的元素就能判斷是否為執行container的命令行。\nParent Process run()這個function定義了parent process的行為。在這裡，我們會需要做到以下這幾件事：\n創建一個child process - 這個child process將會是這個container裡面的PID 1 process。 使用這個child process執行命令行中，run後面的指令。 為這個child process創建一個新的PID namespace、UTS namespace （實際上大家使用的container技術如Docker，可能會有更多的namespace來做更加嚴實的隔離，在這裡我們先使用兩個namespace作為實驗） func run() { cmd := exec.Command(\u0026#34;/proc/self/exe\u0026#34;, append([]string{\u0026#34;child\u0026#34;}, os.Args[2:]...)...) cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr //create a namespace and new pid cmd.SysProcAttr = \u0026amp;syscall.SysProcAttr{ Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID, } must(cmd.Run()) } 這邊可以注意到我使用exec.Command呼叫了一個child process，這個child process會使用我們剛剛編譯過的臨時執行檔，而且會創建一個新的命令行指令，且以\u0026quot;child\u0026quot;作為開頭，而後面的arguments則會與parent process的指令相同。\n講的可能有些饒口，意思是，當parent process執行的命令行指令為go run main.go run /bin/bash，那麼現在這個child process則會執行/tmp/go-build3547203082/b001/exe/test child /bin/bash。\n到這邊，我們就可以開始定義child process接下來的行為了。\nChild Process (container中的PID 1 Process) 我們可以先在這個function埋一個log，來看這個child process的PID是否真的為1。\nfmt.Printf(\u0026#34;running %v as PID %d\\n\u0026#34;, os.Args[2:], os.Getpid()) 在child()function裡面，可以實際來執行剛剛所輸入的命令行指令/bin/bash。\ncmd := exec.Command(os.Args[2], os.Args[3:]... ) cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr must(cmd.Run()) 完成後，到這裡我們就可以先試跑看看程式碼了。 可以觀察到，/bin/bash不僅被執行，且PID真的是1！接著，我們可以來試試看現在是否處在container的隔離環境之中：\n我先在這個container裡面，把我的hostname從Sophie-Desktop替換成test-sophie： 退出container之後，在host觀察hostname，會發現hostname並沒有被更動！UTS namespace的確有發揮作用。 到這裡，一個簡易版的container（看似）就完成了。\n編按：這邊要提醒讀者，這裡的hostname之所以能做到隔離，是因為在前面我們加上了UTS namespace syscall.CLONE_NEWUTS，在這次實驗中，我們只有加了PID和UTS這兩個比較好觀察到的namespace，因此只能在Hostname以及PID的部分觀察到隔離的效果！\n檔案路徑與Process的可見性 為什麼上一個章節會說「看似」完成呢？實際上，我們再回到container中，會發現我們看得到host的檔案以及正在運行的process： 這並不符合我們對於container的理解，對吧？\n我們先來釐清一下為什麼會觀察到這樣的狀況。我們剛剛使用namespace創建了獨立的環境，來讓process運行，但實際上以目前的程式碼來看，container所使用的檔案系統和host會是同一個。為了要避免container裡面的process直接去訪問、操作位在host的檔案，我們可以使用先建造一個具有完整Linux系統的子目錄，再使用上一篇所提到chroot來改變container的根目錄位址。\n在Linux環境中，可以使用debootstrap指令來創建這個子目錄，以下指令是以AMD64的晶片架構、Ubuntu 22.04為例。\nsudo apt-get install debootstrap sudo debootstrap --arch=amd64 jammy /home/rootfs http://archive.ubuntu.com/ubuntu/ 接著就能看到在home/rootfs底下，就有一個基本的Linux檔案架構了。回到程式，便可以指定這個路徑，讓它成為container的根目錄。 must(syscall.Chroot(\u0026#34;/home/rootfs\u0026#34;)) must(os.Chdir(\u0026#34;/\u0026#34;)) 編按：如果使用這樣的方式，container對文件的修改會直接影響到host的檔案。最好的方法，還是要回歸上一篇有稍微提及的聯合檔案系統，將文件分為唯讀層、可寫層，所有的修改都應只在可寫層上進行。\n再回頭來說說Process。在Linux中，/proc負責存取系統中每一個process和thread的狀態。回想前一個章節，當我們還在和host共用檔案目錄時，自然也讀取到了host的/proc檔案，連帶讀取到host的所有process了。\n我們剛剛製作的小型Linux檔案系統並沒有包含/proc檔案系統，因此需要手動另外掛載：\nmust(syscall.Mount(\u0026#34;proc\u0026#34;, \u0026#34;proc\u0026#34;, \u0026#34;proc\u0026#34;, 0, \u0026#34;\u0026#34;)) 完成後，再回頭執行ps aux，就會出現預期中的效果了。 結語 此次透過實作一個container，來更加了解container的底層技術。不過，礙於篇幅及時間有限，實做出來的東西僅做實驗使用，並沒有做到非常完整的隔離與資源限制，或許都能成為下次努力精進的方向。\n最後，不免俗地於文末附上完整的程式碼。（當然，僅供學習使用，其嚴謹度當然是不能夠使用在production環境！）\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;os/exec\u0026#34; \u0026#34;syscall\u0026#34; ) func main() { switch os.Args[1] { case \u0026#34;run\u0026#34;: run() case \u0026#34;child\u0026#34;: child() default: panic(\u0026#34;what??\u0026#34;) } } func run() { cmd := exec.Command(\u0026#34;/proc/self/exe\u0026#34;, append([]string{\u0026#34;child\u0026#34;}, os.Args[2:]...)...) cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr //create a namespace and new pid cmd.SysProcAttr = \u0026amp;syscall.SysProcAttr{ Cloneflags: syscall.CLONE_NEWUTS | syscall.CLONE_NEWPID, } must(cmd.Run()) } func child() { fmt.Printf(\u0026#34;running %v as PID %d\\n\u0026#34;, os.Args[2:], os.Getpid()) cmd := exec.Command(os.Args[2], os.Args[3:]... ) cmd.Stdin = os.Stdin cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr //create file system for container must(syscall.Chroot(\u0026#34;/home/rootfs\u0026#34;)) must(os.Chdir(\u0026#34;/\u0026#34;)) //mount /proc must(syscall.Mount(\u0026#34;proc\u0026#34;, \u0026#34;proc\u0026#34;, \u0026#34;proc\u0026#34;, 0, \u0026#34;\u0026#34;)) must(cmd.Run()) } func must(err error) { if err != nil { panic(err) } } Reference 此篇文章受這部影片所啟發：Building a container from scratch in Go - Liz Rice (Microscaling Systems)\n","permalink":"http://localhost:1313/posts/2024/06/build_container_by_go_practice/","summary":"來實作一個container吧 延續上篇的動手用Golang實作一個container - 概念篇，了解container的底層技術是如何實踐之後，我們就可以開始使用Golang來做出屬於我們自己的container了。\n以Docker為例，當我們要啟動一個container的時候，會使用這個指令：\ndocker container run \u0026lt;image-name\u0026gt; [cmd] 以此為發想點，當我想要使用我的程式碼啟動container時，他長得會像這樣子：\ngo run main.go run [cmd] 在這裡，我們會分別定義兩種function：\nrun() : parent process要執行的function。負責創建child process及並配置其運行的環境（如namespace）。 child() : child process要執行的function。負責管理在container環境中，要如何運行用戶端所指定的命令。 而must()function則會作為error handler使用。\nfunc main() { switch os.Args[1] { case \u0026#34;run\u0026#34;: run() case \u0026#34;child\u0026#34;: child(); default: panic(\u0026#34;what??\u0026#34;) } } func must(err error) { if err != nil { panic(err) } } 這邊還蠻想提一下os.Args這個指令。os.Args 是Golang裡面用來儲存命令行參數的一個變數。如果我們只單印出os.Args，他會長的像這樣：\nsophie@Sophie-Desktop:~/go-container$ go run test.go run echo \u0026#39;hi\u0026#39; [/tmp/go-build3547203082/b001/exe/test run echo hi] 第一行是我執行Go程式碼的指令，第二行是os.Args印出來的結果。到這裡會發現為什麼go run test.","title":"動手用Golang實作一個container - 實作篇"},{"content":"前言 開始第一份工作以後，真切體會到容器化技術的強大與方便之處，工作中處處離不開container，但自己又真的懂它幫我們做了什麼事情嗎？為了更了解容器化技術的底層原理，那不如就自己來做一個container看看好了！\nCNCF的開發專案大多由Golang寫成，同時做為一個語法簡潔、易讀、擁有強大併發處理能力的語言，我相信使用它來建構容器等系統工具是個好選擇，因此本文將會以Golang作為程式碼的範例。\n什麼時候會需要用到container? 要回答這個問題，我們先來看看Docker官方對於container的解釋：\nA container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another.\n一年前我還是個連docker都沒聽過的程式小白，當我想把網站部署到雲端上時，我的作法就是直接把github的程式碼clone到我的機器上，直接加裝任何程式碼所需要的套件及程式語言。現在回想起來，直接在機器上面運行程式碼實在有太多風險了：錯誤的程式碼可能造成系統損壞、資源過度消耗，甚至是運行的程式碼可能包含惡意程式而導致安全漏洞……除此之外，直接在機器上加裝一堆套件也讓環境變得又髒又亂，更別說在多人協作的開發場合中，建置環境時也很常發生「為什麼程式碼在你的電腦可以跑，我的不行？」的惱人狀況。\n基於以上痛點，我們再回頭來看container的定義：container是一個標準的軟體單位，它把程式碼以及所需要的環境與依賴項目給一起打包，讓整個應用程式可以快速地被搬運到另一個運算環境並且可靠地運行。有了container，我們不但可以避免直接運行程式碼的風險，開發環境與生產環境也都會變得乾淨許多。\n那麼，container是怎麼做到的？以上面的定義來看，container做到了環境打包、隔離，這兩個功能對應到的linux技術即是filesystem以及namespace，除此之外，host也需要去管理與限制container可以使用的資源，而這部分就屬於cgroups的範疇。因此，為了更了解container的底層原理，以下將會敘述這三個技術是怎麼成就container的。\nNamespace 當我們運行一個container時，會發現在container當中，我們只能看到在container裡運行的process，如果先前對container有一些認識，大概會知道container是使用命名空間來做到隔離。\n我們直接來看Linux manual page中對於Namespace的解釋：\nA namespace wraps a global system resource in an abstraction thatmakes it appear to the processes within the namespace that theyhave their own isolated instance of the global resource. Change to the global resource are visible to other processes that are members of the namespace, but are invisible to other processes.\nNamespace透過把global的系統資源封裝，使得處在同個namespace的processes可以看到所處namespace的資源；如果是在別的namespace的process，就也只能看到自己所處的namespace資源，而看不到其他namespace的資源與processes，透過這樣的方式，不同namespace的processes們看起來就像被「隔離」了！\n不過，這只限於被關在container裡面的process，對於host來說，他可以看到每個container裡面所運行的process，只是，以同一個process來說，從host視角所看見的PID，和container裡面的PID又會是不相同的。\n在筆者撰寫文章的當下(2024年)，Linux總共有八種namespaces，分別是Cgroup, IPC, Network, Mount, PID, Time. User, UTS。簡單來說，每個namespace的種類名稱就代表它隔離了甚麼樣的資源。因為等等我們會需要使用Golang建立namespace，在這邊附上Linux manual table對於各個namespace的介紹，待會實作時我們也會使用到這些namespace：\nNamespace Flag Page Isolates Cgroup CLONE_NEWCGROUP cgroup_namespaces(7) Cgroup root directory IPC CLONE_NEWIPC ipc_namespaces(7) System V IPC, POSIX message queues Network CLONE_NEWNET network_namespaces(7) Network devices, stacks, ports, etc. Mount CLONE_NEWNS mount_namespaces(7) Mount points PID CLONE_NEWPID pid_namespaces(7) Process IDs Time CLONE_NEWTIME time_namespaces(7) Boot and monotonic clocks User CLONE_NEWUSER user_namespaces(7) User and group IDs UTS CLONE_NEWUTS uts_namespaces(7) Hostname and NIS domain name Linux manual page中的namespace介紹 cgroup 現在，我們擁有了多個獨立運行的namespace，cgroup則幫我們實現了namespace之間的資源分配與限制。cgroup的全名叫做Control Group，是Linux Kernel的功能之一，它將processes組織到一個有階層的(hierarchical)組別當中，藉此來監控並限制資源的分配，來對CPU, memory等資源做更精細的控制。\ncgroup把每種可以控制的資源定義為一個子系統(subsystem)，每個子系統都會和kernal的其他模組配合來完成資源控制，例如，CPU子系統會和process scheduling module配合來完成CPU的資源配置。\n所以，以CPU和Memory資源為例，我們的cgroup filesystem架構可能會長得像下面這張圖片：\n圖片取自: https://tech.meituan.com/2015/03/31/cgroups.html 可以看到， Cgroup Hierarchiy A使用了cpu以及cpuacct這兩個subsystem，在Hierarchiy A這個階層中，最上層的/cpu_cgrp作為根節點，可以想像成是它握有這個階層的總資源，並再進一步地將資源分配給它下面的子節點， 因此我們可以說，根節點負責：\n資源分配：因為根節點擁有整個階層的資源，它會按照訂定的策略將資源分配下去。 管理cgroup：根節點會監控每個子節點的資源使用情況，確保資源分配的正確性。 設定屬性與限制：根節點可以設置屬性與限制，而繼承根節點的那些子節點必須遵守。例如，根節點可以設置RAM的使用上限，而這個限制會影響所有繼承它的子節點。 而繼承它的子節點則必須遵守這些配置與限制。因此，子節點(在這裡，就是/cgrp1和/cgrp2)才是這個階層當中的實體cgroup，負責將分配到的資源再分配給process來做使用。\n為了要讓使用者可以自行定義各種資源配置，kernal會把cgroup以檔案系統的形式暴露出來，這個文件系統叫做Virtual File System，它讓使用者可以透過編輯、創建文件，就可以控制process的資源使用。\n大家常提到的cgroup v1和v2的比較，主要也是在檔案系統的管理上的差異。cgroup v1會針對不同的subsystem創建不同的資料夾，如果想要使用不同的subsystem，我們就必須切換到不同的資料夾去建立cgroup；但在v2中，所有的subsystem都會合併到同一個掛載點(通常會是/sys/fs/cgroup)，我們只要進入這個資料夾，就可以看到所有subsystem的配置與控制文件，由此可見，v2相較於v1，簡化了管理配置與管理檔案的流程。\ncgroup v2的檔案系統架構，圖片取自:https://blog.kintone.io/entry/2022/03/08/170206 chroot檔案系統隔離 一般說到container的檔案系統技術，第一時間會想到的應該是Union Filesystem(聯合檔案系統)，不過這次實作並沒有引入這項技術(XD)，所以這邊就不多加贅述，不過非常推薦大家閱讀小賴老師的鐵人賽文章，以原理搭配實驗講解得非常清楚：Day 07: 什麼是 overlay2?\n題外話結束，那麼這個小章節就來聊聊我們要怎麼幫檔案系統做到隔離。可能大家看到這段的第一個想法會是：要做到隔離，namespace不就可以幫我們做到了嗎？但實際上，namespace只是控制裡面的process可以看到甚麼，雖然不同的namespace中的process看不見彼此，但是process還是可以訪問host的檔案系統！若是如此，不僅並沒有完全做到隔離環境，還有可能會導致安全上的問題。\n簡單來說，chroot改變了特定process即其child process的根目錄，接著，process就只能夠訪問新的根目錄以及下面的子目錄，不能夠對這個根目錄之外的檔案進行讀取或者修改，這樣就實現了container與host間的檔案系統隔離了。\n實際上要怎麼去製作container所使用的root directory，我們在下一章節會使用Golang來進行實作。\n小小後記 本章節所提到關於namespace, cgroup, filesystem的技術，因為篇幅有限，加上本次主題旨在如何透過Golang實作container，因此在內容部分僅點到為止，沒有更深入的探討。尤其是cgroup，可以探討的概念、原理與使用方法(例如cgroup怎麼再往下分配process資源，以及實際如何使用VFS來管理cgroup資源，都還來不及講到)，我認為還蠻值得再花一篇文章介紹它的(先挖坑給自己跳)。\nReference Docker: Use containers to Build, Share and Run your applications\nLinux manual page: namespace(7)\nLinux manual page: cgroups(7)\nThe Linux Kernal Documentation: CGROUPS\nFive Things to Prepare for Cgroup v2 with Kubernetes\n那些關於 docker 你知道與不知道的事-Day 10: 什麼是 namespace?\nLinux資源管理之cgroups簡介\nLinux manual page: chroot(2)\n","permalink":"http://localhost:1313/posts/2024/05/build_container_by_go/","summary":"前言 開始第一份工作以後，真切體會到容器化技術的強大與方便之處，工作中處處離不開container，但自己又真的懂它幫我們做了什麼事情嗎？為了更了解容器化技術的底層原理，那不如就自己來做一個container看看好了！\nCNCF的開發專案大多由Golang寫成，同時做為一個語法簡潔、易讀、擁有強大併發處理能力的語言，我相信使用它來建構容器等系統工具是個好選擇，因此本文將會以Golang作為程式碼的範例。\n什麼時候會需要用到container? 要回答這個問題，我們先來看看Docker官方對於container的解釋：\nA container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another.\n一年前我還是個連docker都沒聽過的程式小白，當我想把網站部署到雲端上時，我的作法就是直接把github的程式碼clone到我的機器上，直接加裝任何程式碼所需要的套件及程式語言。現在回想起來，直接在機器上面運行程式碼實在有太多風險了：錯誤的程式碼可能造成系統損壞、資源過度消耗，甚至是運行的程式碼可能包含惡意程式而導致安全漏洞……除此之外，直接在機器上加裝一堆套件也讓環境變得又髒又亂，更別說在多人協作的開發場合中，建置環境時也很常發生「為什麼程式碼在你的電腦可以跑，我的不行？」的惱人狀況。\n基於以上痛點，我們再回頭來看container的定義：container是一個標準的軟體單位，它把程式碼以及所需要的環境與依賴項目給一起打包，讓整個應用程式可以快速地被搬運到另一個運算環境並且可靠地運行。有了container，我們不但可以避免直接運行程式碼的風險，開發環境與生產環境也都會變得乾淨許多。\n那麼，container是怎麼做到的？以上面的定義來看，container做到了環境打包、隔離，這兩個功能對應到的linux技術即是filesystem以及namespace，除此之外，host也需要去管理與限制container可以使用的資源，而這部分就屬於cgroups的範疇。因此，為了更了解container的底層原理，以下將會敘述這三個技術是怎麼成就container的。\nNamespace 當我們運行一個container時，會發現在container當中，我們只能看到在container裡運行的process，如果先前對container有一些認識，大概會知道container是使用命名空間來做到隔離。\n我們直接來看Linux manual page中對於Namespace的解釋：\nA namespace wraps a global system resource in an abstraction thatmakes it appear to the processes within the namespace that theyhave their own isolated instance of the global resource.","title":"動手用Golang實作一個container - 概念篇"},{"content":"前言 最近在學習Golang，在架設http server時，發現Golang在v1.8推出了一個叫做Graceful Shutdown的功能，說來慚愧，先前寫的幾個小專案雖然也都是http server，卻沒有使用過像這樣的功能，所以用一篇小文章來記錄一下。\n什麼是Graceful Shutdown? 先想像一下今天已經有一個已經上線的電商網站服務，使用者會在這個網站上面瀏覽、購買商品，當這個網站要升版時，服務就必須暫停 — — 意思是，所有還在網站上進行的交易、連線都會被中斷。若是像這樣強制關閉服務，待下一次服務啟動時，我們可能就會發現資料上會有預期外的錯誤與差異。\nGraceful Shutdown直譯來說就是「優雅地關機」，這個意思是，當伺服器收到終止的指令後，如果手上還有正在執行的process，它會先處理完，之後才會真的關閉服務，這麼作不僅可以保障資料的一致與完整性，我們也不需要害怕突然中止程式可能會導致非預期的錯誤。\n普通的HTTP server 以下我會先介紹「沒有」Graceful Shutdown機制且強制關閉服務的觀察現象，在這邊，HTTP server的實作會透過Gin framework來實現。\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func main() { log.Println(\u0026#34;starting server...\u0026#34;) router := gin.Default() router.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { time.Sleep(10 * time.Second) c.String(http.StatusOK, \u0026#34;hello there\u0026#34;) }) srv := \u0026amp;http.Server{ Addr: \u0026#34;:8080\u0026#34;, Handler: router, } srv.ListenAndServe() } 根據以上設計，當我們前往localhost:8080後，等十秒會收到從server回傳的字串；若我們在這十秒內強制中止server，client連線也就會跟著被強制中斷，導致終端機噴出以下錯誤：\n* Empty reply from server * Closing connection 0 curl: (52) Empty reply from server Graceful Shutdown in HTTP server 前一次實驗的錯誤是因為當我們結束server服務時，還有一個client連線還沒有收到預期的回覆，卻被強制中斷連線所導致的錯誤。\n而Golang推出的Graceful Shutdown功能，則是當我們結束server服務時，他會先把連接阜給關閉，接著，對於剩下還沒執行完的連線，會等待他們執行完畢後並一一關閉。\n首先我們一樣先定義好server的連接阜以及router。\nlog.Println(\u0026#34;starting server...\u0026#34;) router := gin.Default() router.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { c.JSON(http.StatusOK, gin.H{ \u0026#34;hello\u0026#34;: \u0026#34;sophie\u0026#34;, }) }) srv := \u0026amp;http.Server{ Addr: \u0026#34;:8080\u0026#34;, Handler: router, } 創建一個在背景運行的goroutine，並在goroutine裡執行server 服務。這麼作是為了不要讓server的運行與後續我們要讓client連線執行完畢的後續流程互相干擾。\ngo func() { if err := srv.ListenAndServe(); err != nil \u0026amp;\u0026amp; err != http.ErrServerClosed { log.Fatalf(\u0026#34;Failed to initialize server: %v\\n\u0026#34;, err) } }() 宣告一個接收os.Signal訊號的channel，如果系統的SIGINT或者SIGTERM訊號被發出，就會關閉quit通道。\n換句話說，如果關閉服務的訊號一直沒有被發出，channel就會一直卡在那邊，以維持正常運作的狀態；當訊號發出後，通道被關閉，後續流程就會被啟動。\n編按：當我們直接kill process時，會收到SIGINT訊號；若這個服務包在docker中，而我們執行docker rm 時，會收到SIGTERM訊號。\nquit := make(chan os.Signal) signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM) \u0026lt;-quit 當關閉訊號發出後，我們傳入一個context，讓程式等待十秒的時間（這邊的等待時間可以根據自己需求作更新），同時，我們將這個context傳入Shutdown函式，讓程式可以在規定的時間內完成所有正在執行的請求，如果規定的時間到了，仍有請求尚未完成，那麼就會強制關閉連線。\n也就是說，假設我們設定的時間不夠久，整個作法就還是強制關閉連線，因此在時間設定上，必須考量到自身server處理資料的時間。\nctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) //確保程式結束後，釋放相關資源 defer cancel() log.Println(\u0026#34;Shutting down server...\u0026#34;) if err := srv.Shutdown(ctx); err != nil { log.Fatalf(\u0026#34;Server forced to shutdown: %v\\n\u0026#34;, err) } 大功告成，整體的程式碼可以參照以下：\npackage main import ( \u0026#34;context\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;os/signal\u0026#34; \u0026#34;syscall\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func main() { log.Println(\u0026#34;starting server...\u0026#34;) router := gin.Default() router.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { time.Sleep(10 * time.Second) c.JSON(http.StatusOK, gin.H{ \u0026#34;hello\u0026#34;: \u0026#34;sophie\u0026#34;, }) }) srv := \u0026amp;http.Server{ Addr: \u0026#34;:8080\u0026#34;, Handler: router, } go func() { if err := srv.ListenAndServe(); err != nil \u0026amp;\u0026amp; err != http.ErrServerClosed { log.Fatalf(\u0026#34;Failed to initialize server: %v\\n\u0026#34;, err) } }() log.Printf(\u0026#34;Listening on port %v\\n\u0026#34;, srv.Addr) quit := make(chan os.Signal) signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM) \u0026lt;-quit ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second) defer cancel() log.Println(\u0026#34;Shutting down server...\u0026#34;) if err := srv.Shutdown(ctx); err != nil { log.Fatalf(\u0026#34;Server forced to shutdown: %v\\n\u0026#34;, err) } } 使用改版後的程式碼運行後，會發現第一次實驗出現的錯誤消失了，並且成功拿到從server端回傳的字串。\nserver 中途shutdown後，client端還是有拿到server回傳的字串 server端接收到關閉訊號後，有多等待10秒，並處理完來自client端的request Reference Go by Example: Signals https://gobyexample.com/signals\n[Go 教學] 什麼是 graceful shutdown? https://blog.wu-boy.com/2020/02/what-is-graceful-shutdown-in-golang/\nGin Web Framework: Graceful restart or stop https://gin-gonic.com/docs/examples/graceful-restart-or-stop/\n","permalink":"http://localhost:1313/posts/2024/05/go_graceful_shotdown/","summary":"前言 最近在學習Golang，在架設http server時，發現Golang在v1.8推出了一個叫做Graceful Shutdown的功能，說來慚愧，先前寫的幾個小專案雖然也都是http server，卻沒有使用過像這樣的功能，所以用一篇小文章來記錄一下。\n什麼是Graceful Shutdown? 先想像一下今天已經有一個已經上線的電商網站服務，使用者會在這個網站上面瀏覽、購買商品，當這個網站要升版時，服務就必須暫停 — — 意思是，所有還在網站上進行的交易、連線都會被中斷。若是像這樣強制關閉服務，待下一次服務啟動時，我們可能就會發現資料上會有預期外的錯誤與差異。\nGraceful Shutdown直譯來說就是「優雅地關機」，這個意思是，當伺服器收到終止的指令後，如果手上還有正在執行的process，它會先處理完，之後才會真的關閉服務，這麼作不僅可以保障資料的一致與完整性，我們也不需要害怕突然中止程式可能會導致非預期的錯誤。\n普通的HTTP server 以下我會先介紹「沒有」Graceful Shutdown機制且強制關閉服務的觀察現象，在這邊，HTTP server的實作會透過Gin framework來實現。\npackage main import ( \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/gin-gonic/gin\u0026#34; ) func main() { log.Println(\u0026#34;starting server...\u0026#34;) router := gin.Default() router.GET(\u0026#34;/\u0026#34;, func(c *gin.Context) { time.Sleep(10 * time.Second) c.String(http.StatusOK, \u0026#34;hello there\u0026#34;) }) srv := \u0026amp;http.Server{ Addr: \u0026#34;:8080\u0026#34;, Handler: router, } srv.ListenAndServe() } 根據以上設計，當我們前往localhost:8080後，等十秒會收到從server回傳的字串；若我們在這十秒內強制中止server，client連線也就會跟著被強制中斷，導致終端機噴出以下錯誤：\n* Empty reply from server * Closing connection 0 curl: (52) Empty reply from server Graceful Shutdown in HTTP server 前一次實驗的錯誤是因為當我們結束server服務時，還有一個client連線還沒有收到預期的回覆，卻被強制中斷連線所導致的錯誤。","title":"Golang中的Graceful Shutdown"},{"content":"iTHOME自2022年舉辦第一場SRE Conference，今年已是第三屆，而這也是我從AppWorks School後端班畢業後參加的第一場技術研討會。做為一個剛從後端領域跨足到SRE的新手來說，此行不僅看到各個公司在導入SRE以及kubernetes的評估與考量之外，透過工作坊的動手做，了解了kubernetes絕對不是僅止於撰寫yaml檔而已。感嘆著這條路的水果然很深之外，更因為還有許多地方可以探索而感到非常興奮。\n此篇文章主要是參加幾場演講下來的速記，因為有些演講的筆記較多，可能會分為兩到三篇來撰寫，同時也會以每場演講作為主題劃分。\nData Architecture and Analysis about OpenTelemetry Observability 講者：蘇揮原 (Mars), TrendMicro\n講者一開始先從趨勢科技的自有產品 - Vision One作為引言，當產品從\u0026quot;Security Tool\u0026quot;逐漸壯大成一個\u0026quot;Cybersecurity Platform\u0026quot;時，那我們該怎麼去管理這些服務？我們可以從下面那張圖看到，Vision One透過單一的平台服務來偵測、預防與應對來自不同地方的資安攻擊與風險，並搭配自動化與人工智慧來落實全方位的資安管理。\n圖片擷取自趨勢科技官方網站：https://www.trendmicro.com/zh_tw/business/products/one-platform.html 那麼，有這麼多的服務都運行在單一的平台上面，勢必得做好管理。講者在這裡提到了兩個名詞：Proactive monitoring以及Observability。我會佔用以下小小的篇幅來大致講述這兩個名詞概念。\n許多針對監控相關的產品網站都提到了proactive monitoring的概念，而我在Datadog官方網站上找到proactive monitoring的定義為： Proactive monitoring is key to flagging potential issues with your applications and infrastructure early, enabling you to respond quickly and reduce downtime.\n意思即是，主動監控是及早發現應用程式與基礎架構潛在問題的關鍵，它幫助我們能快速針對這些問題做出反應，減少server downtime。\n在這裡講者也針對proactive monitoring拋出了一個概念：在用戶發現前先發現問題。\n另一方面，與Proactive Monitoring相互輝映的名詞及是Observebility，以我自己的邏輯來看，我們已經了解到了Proactive Monitoring的好處，那我們該怎麼去做到實際上的監控？第一，我們的系統必須具備可以被觀測(Observable)的能力；再來，透過這些觀測到的資訊，它應該要能幫助我們了解目前系統或者服務的狀態，且我們能有效利用這些資訊來做出適當的判斷。\n在這裡也一併附上CNCF(Cloud Native Computing Foundation)對於Observability的解釋：\nObservability is a system property that defines the degree to which the system can generate actionable insights. It allows users to understand a system’s state from these external outputs and take (corrective) action.\nObservable systems yield meaningful, actionable data to their operators, allowing them to achieve favorable outcomes (faster incident response, increased developer productivity) and less toil and downtime.\nConsequently, how observable a system is will significantly impact its operating and development costs.\nOpenTelemetry Concept \u0026amp; Data Architecture 鋪陳到這裡總算進入到正題了！如同上面提到的，監控即是指我們怎麼去利用手上拿到的資料，做出最適當的判斷，在用戶發現之前提早偵測問題去解決。\nOpenTelemetry（簡稱OTel）是雲原生的可觀測性框架，協助開發者蒐集並導出Observability Signal(Metric, Log, Trace)，他提供標準化的API及SDK來降低蒐集數據的困難度，進而讓開發者們能更方便地進行後續的資料分析以了解系統的性能與行為。\n編按： 為了做到軟體服務的可觀測性(Observebility)，服務必須要能夠發出如Metric, Log, Trace等資料，在這裡，我們稱做這些資料為Observability Signal，OpenTelemetry即是透過蒐集這些資料，把這些資料轉送到後端，進而達成監控的目的。\n既然我們已經了解了OpenTelemetry的功能與目的，那麼就可以把它與其他監控工具如Prometheus, Grafana等串聯在一起，建構完整的Data Architecture。\n在這邊講者有秀出在公司實踐過的系統架構，在這篇筆記就不多加贅述，在這部分其實講者有引入幾個在設計這種data pipeline時可以有的幾個思路：\n當資料量非常龐大的時候，要怎麼做才能有效率地進行資料查詢與分析？　＞　可能需要尋找一個適當的資料倉儲？ Throughtput: 系統要可以掌握大量的資料擷取(data ingestion)與查詢(queries) Analysis: 可以根據不同的情境輕鬆地產出對應的資料分析，甚至這些資料要能餵給LLM進行機器學習 Cost: 成本控制 綜合以上的思路與考量，我們可以選擇OLAP Data Warehouse來有效處理大量資料並做對應的數據呈現，至於為什麼選擇OLAP，請看下個章節的介紹。\nOLAP Data Warehouse \u0026amp; ClickHouse 每種資料庫進行數據儲存的方式不盡相同，不同的儲存方式都有其適合的情境，當資料量一大、系統負載不斷增加時，要選擇何種的儲存方式就顯得非常重要。\nOLAP(Online Analyticla Processing, 線上分析處理)通常用於儲存和處理大量資料，以下簡單列點幾項OLAP的特性以及擅長處理之情境：\n當大多數的請求都是以讀取資料為主時 查詢時間非常快速 可以從資料庫中快速讀取很大筆的行資料 高吞吐量(throughtput) 不太需要資料庫的transaction特性 資料需要可以被篩選或聚合，讓資料庫的查詢結果比來源資料還要小 綜合以上，當資料庫儲存資料的結構上採取Column-Based時，就可以很好地實現或應對以上情境，這是因為資料採取分欄位儲存的方式，當我們只需要提取某些欄位的資料時，我們只需要讀取特定的欄位，而不需要讀取整張資料表。\n{ \u0026#34;id\u0026#34;: [1, 2, 3], \u0026#34;name\u0026#34;: [\u0026#34;John Doe\u0026#34;, \u0026#34;Jane Smith\u0026#34;, \u0026#34;Mike Johnson\u0026#34;], \u0026#34;age\u0026#34;: [30, 25, 28], \u0026#34;email\u0026#34;: [\u0026#34;johndoe@example.com\u0026#34;, \u0026#34;janesmith@example.com\u0026#34;, \u0026#34;mikejohnson@example.com\u0026#34;], \u0026#34;occupation\u0026#34;: [\u0026#34;Software Developer\u0026#34;, \u0026#34;Data Analyst\u0026#34;, \u0026#34;Product Manager\u0026#34;] } column-base data structure 基於OLAP以上的特性，它時常與Data Warehouse結合使用，來增強Data Warehouse的分析能力與效能。而我們可以結合OLAP servers 以及Data Warehouse來完整從資料收集一直到產生儀表板的流程。\n圖片出處：Readings in Database Systems, 3rd Edition　Stonebraker \u0026 Hellerstein, eds. 透過上面的示意圖，我們也可以說從一開始的information sources到Data Warehouse這一段，採用的是ETL(Extract, Transform, Load)，主要著重在技術邏輯(Technical Logic)；而從Dataware House一直到用戶端產生儀表板或者數據分析的這一段，採用的是ELT(Extract, Load, Transform)，這邊我自己的理解是先把資料撈出來，再根據用戶端的商業邏輯與需求，將資料進行轉換並產生適當的圖表，因此相對於前半段，這邊主要是根據商業邏輯(Business Logic)做出的流程。\n用於OLAP情境的資料庫系統非常多，在這場演講中，講者特別介紹了ClickHouse這項產品，因此我稍微搜尋了一下ClickHouse的介紹以及優勢。\nClickHouse® is a high-performance, column-oriented SQL database management system (DBMS) for online analytical processing (OLAP). It is available as both an open-source software and a cloud offering.\nClickHouse 官網介紹 作為一個column-based 的資料庫管理系統，除了上述提到的column-based資料庫應該具備的優勢與特性以外，他也支援OpenTelemetry的exporter - 沒錯，就是前面那個章節提到的雲原生框架；在性能方面，ClickHouse透過分布式查詢、並行(Concurrency)處理、資料壓縮等方式，來完成更有效率的資料庫查詢，同時它也支持基於ANSI SQL standard的查詢語言。\nClickHouse Keeper ClickHouse 資料庫支援分散式與複製資料的功能，其方法是透過將資料切成多個片段(shards)，而這些資料片段則會分布在不同的資料庫節點上，而多個節點則可以集合成一個叢集(cluster)。 ClickHouse的官方網站聲稱\u0026quot;ClickHouse Keeper is a drop-in replacement for ZooKeeper\u0026quot;，在ClickHouse cluster的不同節點之間，它確保集群中的各個節點可以保持一致性且能互相協調工作，且相較於Zookeeper，對於同樣的資料量來說，它在Memory, CPU等效能方面的表現都較Zookeeper還要來得好。\nClickHouse keeper與Zookeeper在memory usage的表現差異 圖片出處：ClickHouse官網 ClickHouse Keeper使用兩種不同的方式來做讀/寫操作。在叢集當中，每個節點上面都會有一個local table，當我們在寫入資料時，則會直接把資料寫入某個節點上面的local table。但當我們要做讀操作時，則會先經過一個叫做Distributed Table的虛擬表，它會把各個節點上面的local table資料給匯集起來，這樣即使我們不個別探訪每個node上面的local table，也能讀取到完整的資料。\nSystem Design (ClickHouse as storage) 綜上所述，ClickHouse協助我們處理需要大量查詢、大量讀取資料的情境，因此，當我們要設計一個Monitoring pipeline時，可以結合前面說到的ETL, ELT的觀念，並將原先收集資料的Prometheus取代成ClickHouse： Kafka + OpenTelemetry (ETL, 使用Kafka處理data streaming並使用OpenTelemetry蒐集資料) -\u0026gt; ClickHouse Cluster(ELT) -\u0026gt; Grafana (呈現資料)\n心得 在AppWorks School時的個人專案剛好就做了一個監控系統，在其中的幾場面試中，也常被問到該如何處理大量寫入、大量讀取的情境。而在實際的業務需求上，常聽到的監控系統不外乎就是Prometheus串聯Grafana來做使用，但當需要監控的資料量非常龐大時，要怎麼提升處理資料的效能便是一件值得團隊來思考的問題。在這場演講中學習了不同的資料處理方式，以及一個團隊在面臨效能瓶頸時，是如何在不斷地調整架構與測試中，找到最適合團隊的設計。\n講者最後還有提到ClickHouse的另一個功能 - Feature Stores，它用來儲存、查詢、管理機器學習的特徵，但在這部分本文章就不花篇幅敘述，可以參考官方ClickHouse的介紹影片：\nReference 趨勢科技 - Trend Vision One https://www.trendmicro.com/zh_tw/business/products/one-platform.html\nOLAP 和 OLTP 有什麼區別？ https://aws.amazon.com/tw/compare/the-difference-between-olap-and-oltp/\nWhat is OLAP and how can you use it in data warehousing? https://www.linkedin.com/advice/0/what-olap-how-can-you-use-data-warehousing-skills-data-management-9fpnc\nClickHouse Document https://clickhouse.com/docs/en/intro\nPowering Feature Stores with ClickHouse https://clickhouse.com/blog/powering-featurestores-with-clickhouse\n踏上 MLOps 之路：從 Applied Data Scientist 到 MLOps 的轉變與建構：Day 18 Feature Store https://ithelp.ithome.com.tw/articles/10324947?sc=rss.iron\n","permalink":"http://localhost:1313/posts/2024/05/sre_conference_1/","summary":"iTHOME自2022年舉辦第一場SRE Conference，今年已是第三屆，而這也是我從AppWorks School後端班畢業後參加的第一場技術研討會。做為一個剛從後端領域跨足到SRE的新手來說，此行不僅看到各個公司在導入SRE以及kubernetes的評估與考量之外，透過工作坊的動手做，了解了kubernetes絕對不是僅止於撰寫yaml檔而已。感嘆著這條路的水果然很深之外，更因為還有許多地方可以探索而感到非常興奮。\n此篇文章主要是參加幾場演講下來的速記，因為有些演講的筆記較多，可能會分為兩到三篇來撰寫，同時也會以每場演講作為主題劃分。\nData Architecture and Analysis about OpenTelemetry Observability 講者：蘇揮原 (Mars), TrendMicro\n講者一開始先從趨勢科技的自有產品 - Vision One作為引言，當產品從\u0026quot;Security Tool\u0026quot;逐漸壯大成一個\u0026quot;Cybersecurity Platform\u0026quot;時，那我們該怎麼去管理這些服務？我們可以從下面那張圖看到，Vision One透過單一的平台服務來偵測、預防與應對來自不同地方的資安攻擊與風險，並搭配自動化與人工智慧來落實全方位的資安管理。\n圖片擷取自趨勢科技官方網站：https://www.trendmicro.com/zh_tw/business/products/one-platform.html 那麼，有這麼多的服務都運行在單一的平台上面，勢必得做好管理。講者在這裡提到了兩個名詞：Proactive monitoring以及Observability。我會佔用以下小小的篇幅來大致講述這兩個名詞概念。\n許多針對監控相關的產品網站都提到了proactive monitoring的概念，而我在Datadog官方網站上找到proactive monitoring的定義為： Proactive monitoring is key to flagging potential issues with your applications and infrastructure early, enabling you to respond quickly and reduce downtime.\n意思即是，主動監控是及早發現應用程式與基礎架構潛在問題的關鍵，它幫助我們能快速針對這些問題做出反應，減少server downtime。\n在這裡講者也針對proactive monitoring拋出了一個概念：在用戶發現前先發現問題。\n另一方面，與Proactive Monitoring相互輝映的名詞及是Observebility，以我自己的邏輯來看，我們已經了解到了Proactive Monitoring的好處，那我們該怎麼去做到實際上的監控？第一，我們的系統必須具備可以被觀測(Observable)的能力；再來，透過這些觀測到的資訊，它應該要能幫助我們了解目前系統或者服務的狀態，且我們能有效利用這些資訊來做出適當的判斷。\n在這裡也一併附上CNCF(Cloud Native Computing Foundation)對於Observability的解釋：\nObservability is a system property that defines the degree to which the system can generate actionable insights.","title":"2024 SRE Conference Record (1)"},{"content":" Sophie Hsu Site Reliability Engineer。喜歡Linux、Container、Monitoring、系統設計等相關議題，也喜歡以開發者的角度來解決維運問題。最近正在努力學習Python與Golang。\n","permalink":"http://localhost:1313/about/","summary":"Sophie Hsu Site Reliability Engineer。喜歡Linux、Container、Monitoring、系統設計等相關議題，也喜歡以開發者的角度來解決維運問題。最近正在努力學習Python與Golang。","title":"About Me"}]